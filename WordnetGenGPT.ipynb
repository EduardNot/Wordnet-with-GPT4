{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/erudi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import datetime\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from estnltk.wordnet import Wordnet as EstWordnet\n",
    "import estnltk as et\n",
    "nltk.download('wordnet')\n",
    "estwn = EstWordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"API_TYPE\")\n",
    "openai.api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_base = os.getenv(\"API_BASE\")\n",
    "openai.api_version = os.getenv(\"API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_ID = 1\n",
    "\n",
    "\n",
    "def get_initial_prompt_xml(is_est=False):\n",
    "    return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation. You will be given a word and you have to give all defenitions of the word and give an example.\n",
    "            {\"You will be given the word in Estonian. Meaning and example must be in Estonian.\" if is_est else \"\"}\n",
    "            The output must contain only XML formatted answer. The XML must look like this:\n",
    "            <definitions>\n",
    "                <definition>\n",
    "                    <word>[Given word]</word>\n",
    "                    <type>[adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]</type>\n",
    "                    <meaning>[Meaning of the word]</meaning>\n",
    "                    <example>[An example sentece with given word]</example>\n",
    "                </definition>\n",
    "            </definitions>\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_prompt(is_est=False):\n",
    "    return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation. \n",
    "            You will be given a word and you must give all defenitions of the word.\n",
    "            The output must contain only plain text and must contain given word, word type, meaing of the word and and example separeted by a new line. An example of the output:\n",
    "            Word: [Given word]\n",
    "            Type: [adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]\n",
    "            Meaning: [Meaning of the word]\n",
    "            Example: [An example sentece with given word]\n",
    "            Do not include any other information.\"\"\"\n",
    "    # return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation.\n",
    "    #         You will be given a word and you have to give all defenitions that are in Python NLTK library's WordNet. DO NOT INCLUDE people and places.\n",
    "    #         {\"You will be given the word in Estonian. Meaning and example must be in Estonian.\" if is_est else \"\"}\n",
    "    #         The output must contain only plain text and must contain given word, word type, meaing of the word and and example separeted by a new line. An example of the output:\n",
    "    #         [Given word]\n",
    "    #         [adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]\n",
    "    #         [Meaning of the word]\n",
    "    #         [An example sentece with given word]\n",
    "    #         Do not include any other information in the output even \"Meaning\" and \"Example\" words.\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_prompt_est():\n",
    "    return f\"\"\" Sa oled kõrgelt kvalifitseeritud keele mõistmise ja WordNeti genereerimise AI.\n",
    "                Sulle antakse sõna ja sa pead andma kõik selle sõna definitsioonid.\n",
    "                Väljund peab sisaldama ainult tavalist teksti ja peab sisaldama, uue reaga eraldatud: antud sõna, sõna tüüpi, sõna tähendust ja näide. Väljundi näide:\n",
    "                Sõna: [Antud sõna]\n",
    "                Tüüp: [omadussõna/abiverb/sidesõna/määrsõna/asesõna/nimisõna/palind/tegusõna]\n",
    "                Tähendus: [Sõna tähendus]\n",
    "                Näide: [Näide lause antud sõnaga]\n",
    "                Ära lisa väljundisse muud informatsiooni.\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str_xml(word):\n",
    "    return f\"\"\"Now you will be given the following fields: id, word, type, meaning and example. \n",
    "    You will have to give exact {word}s {\"that are in WordNet in Python NLTK library\" if False else \"\"}.\n",
    "    The output must contain only XML. Here is an example of what XML must look like:\n",
    "\n",
    "            <{word}s>\n",
    "                <{word}>[{word} of word 1]</{word}>\n",
    "                <{word}>[{word} of word 2]</{word}>\n",
    "                <{word}>[{word} of word 3]</{word}>\n",
    "                ...\n",
    "            </{word}s>\n",
    "\n",
    "    The XML is just an example, there can be more or less {word}s for each word. If there are no {word}s, just leave the {word}s tag empty.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str(word):\n",
    "    return f\"\"\"Now you will be given the following fields: word, type, meaning and example.\n",
    "    You will have to give exact {word}s{\" that are in WordNet in Python NLTK library\" if False else \"\"}. \n",
    "    The output must contain only plain text. Here is an example of what the text must look like:\n",
    "            [First {word}]\n",
    "            [Second {word}]\n",
    "            [Third {word}]\n",
    "            ...\n",
    "    If there are no {word}s, just leave the output empty.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str_est(word):\n",
    "    return f\"\"\"Nüüd antakse sulle järgmised väljad: sõna, tüüp, tähendus ja näide.\n",
    "    Sa pead andma täpsed {word}id, mis on Pythoni EstNLTK WordNetis.\n",
    "    Väljund peab sisaldama ainult tavalist teksti. Siin on näide, kuidas tekst peab välja nägema:\n",
    "            [Esimene {word}]\n",
    "            [Teine {word}]\n",
    "            [Kolmas {word}]\n",
    "            ...\n",
    "    Kui sõnu pole, jäta väljund tühjaks.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "relations = ['synonym', 'hyponym', 'meronym', 'antonym']\n",
    "relations_est = ['sünonüüm', 'hüponüüm', 'meronüüm', 'antonüüm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price = 0\n",
    "def openai_api_calculate_cost(usage, model=\"gpt-4-1106-preview\"): # https://community.openai.com/t/how-to-calculate-the-cost-of-a-specific-request-made-to-the-web-api-and-its-reply-in-tokens/270878/15\n",
    "    global total_price\n",
    "    pricing = {\n",
    "        'gpt-3.5-turbo-1106': {\n",
    "            'prompt': 0.001,\n",
    "            'completion': 0.002,\n",
    "        },\n",
    "        'gpt-4-1106-preview': {\n",
    "            'prompt': 0.01,\n",
    "            'completion': 0.03,\n",
    "        },\n",
    "        'gpt-4': {\n",
    "            'prompt': 0.03,\n",
    "            'completion': 0.06,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model_pricing = pricing[model]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model specified\")\n",
    "\n",
    "    prompt_cost = usage.prompt_tokens * model_pricing['prompt'] / 1000\n",
    "    completion_cost = usage.completion_tokens * \\\n",
    "        model_pricing['completion'] / 1000\n",
    "\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    # round to 6 decimals\n",
    "    total_cost = round(total_cost, 6)\n",
    "\n",
    "    # print(\n",
    "    #     f\"\\nTokens used:  {usage.prompt_tokens:,} prompt + {usage.completion_tokens:,} completion = {usage.total_tokens:,} tokens\")\n",
    "    # print(f\"Total cost for {model}: ${total_cost:.4f}\\n\")\n",
    "    total_price += total_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def is_person(words):\n",
    "    words_checked = [part[0].isupper()\n",
    "                     for word in words for part in word.split('_')]\n",
    "    return all(words_checked)\n",
    "\n",
    "\n",
    "def remove_short_words(words):\n",
    "    return [word for word in words if len(word) > 2]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/53416780/how-to-convert-token-list-into-wordnet-lemma-list-using-nltk\n",
    "def convert_to_lemma(sentence):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            lemmas += [synset.lemmas()[0].name()\n",
    "                       for synset in wn.synsets(token)]\n",
    "        except:\n",
    "            lemmas += [token]\n",
    "    return set(lemmas)\n",
    "\n",
    "\n",
    "def convert_to_lemma_est(sentence_ls):\n",
    "    sentence = ' '.join(sentence_ls)\n",
    "    text = et.Text(sentence)\n",
    "    lemmas_layer = text.tag_layer().morph_analysis.lemma\n",
    "    lemmas = [word for lemmas_list in lemmas_layer for word in lemmas_list]\n",
    "    return set(lemmas)\n",
    "\n",
    "\n",
    "def find_wordnet_synset(word, definition):\n",
    "    overlap = 0\n",
    "    most_overlap_synset = None\n",
    "    parsed_def = remove_short_words(definition.lower().split())\n",
    "    parsed_def_lemma = convert_to_lemma(parsed_def)\n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        if is_person(synset.lemma_names()):\n",
    "            continue\n",
    "        actual_def = remove_short_words(synset.definition().lower().split())\n",
    "        actual_def_lemma = convert_to_lemma(actual_def)\n",
    "        overlap_temp = len(set(actual_def).intersection(set(parsed_def)))\n",
    "        overlap_temp_lemma = len(\n",
    "            actual_def_lemma.intersection(parsed_def_lemma))\n",
    "        overlap_temp += overlap_temp_lemma\n",
    "        if overlap_temp > overlap:\n",
    "            overlap = overlap_temp\n",
    "            most_overlap_synset = synset\n",
    "    return most_overlap_synset\n",
    "\n",
    "\n",
    "def get_word_synset(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas()\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms()\n",
    "        case 'meronym':\n",
    "            return synset.part_meronyms()\n",
    "        case 'antonym':\n",
    "            return synset.lemmas()[0].antonyms()\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms()\n",
    "        case 'holonyms':\n",
    "            return synset.member_holonyms()\n",
    "        case 'pertainyms':\n",
    "            return synset.pertainyms()\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")\n",
    "\n",
    "\n",
    "def find_wordnet_synset_est(word, definition):\n",
    "    overlap = -1\n",
    "    most_overlap_synset = None\n",
    "    parsed_def = remove_short_words(definition.lower().split())\n",
    "    parsed_def_lemma = convert_to_lemma_est(parsed_def)\n",
    "    for i, synset in enumerate(estwn[word]):\n",
    "        actual_def = remove_short_words(synset.definition.lower().split())\n",
    "        actual_def_lemma = convert_to_lemma_est(actual_def)\n",
    "        overlap_temp = len(set(actual_def).intersection(set(parsed_def)))\n",
    "        overlap_temp_lemma = len(\n",
    "            actual_def_lemma.intersection(parsed_def_lemma))\n",
    "        overlap_temp += overlap_temp_lemma\n",
    "        if overlap_temp > overlap:\n",
    "            overlap = overlap_temp\n",
    "            most_overlap_synset = synset\n",
    "    return most_overlap_synset\n",
    "\n",
    "\n",
    "def get_word_synset_est(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms\n",
    "        case 'meronym':\n",
    "            return synset.meronyms\n",
    "        case 'antonym':\n",
    "            return synset.get_related_synset('antonym')\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms\n",
    "        case 'holonyms':\n",
    "            return synset.holonyms\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_from_prompt(msg, tags):\n",
    "    completion = None\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(deployment_id=\"gec\", model=\"gpt-4-1106-preview\", messages=msg)\n",
    "        answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # print(completion)\n",
    "        # if check_XML_validity(answer) and check_tags_XML(answer, tags):\n",
    "        #     break\n",
    "\n",
    "    except openai.error.ServiceUnavailableError:\n",
    "        pass\n",
    "        # Happens sometimes, just asking again usually helps\n",
    "\n",
    "    except openai.error.APIError:\n",
    "        pass\n",
    "        # Haven't looked, why does it happen, asking again helps usually\n",
    "    except KeyError as e:\n",
    "        if completion[\"choices\"][0][\"finish_reason\"] == \"content_filter\":\n",
    "            pass\n",
    "            # Some filter, happens even when nothing is wrong with the input, asking again might help\n",
    "\n",
    "    except openai.error.InvalidRequestError:\n",
    "        pass\n",
    "        # Aslo something related to input text\n",
    "\n",
    "    except openai.error.RateLimitError:\n",
    "        time.sleep(3)\n",
    "        # The error message said, that it's better to wait three seconds and try again\n",
    "    if completion is not None:\n",
    "        openai_api_calculate_cost(completion[\"usage\"])\n",
    "    return completion[\"choices\"][0][\"message\"]\n",
    "\n",
    "def check_XML_validity(xml_str):\n",
    "    try:\n",
    "        ET.fromstring(xml_str)\n",
    "        return True\n",
    "    except ET.ParseError:\n",
    "        return False\n",
    "    \n",
    "def check_tags_XML(xml_str, tags):\n",
    "    try:\n",
    "        for k, v in tags.items():\n",
    "            for elem in ET.fromstring(xml_str).iter(k):\n",
    "                for tag in v:\n",
    "                    if elem.find(tag) is None:\n",
    "                        return False\n",
    "        return True\n",
    "    except ET.ParseError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_xml(prompt=\"crane\", is_est=False):\n",
    "    global WORD_ID\n",
    "\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\":  get_initial_prompt_xml(is_est)},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "    \n",
    "    answer = gen_from_prompt(messages, {'definition' : ['word', 'type', 'meaning', 'example']})\n",
    "    \n",
    "    messages.append(dict(answer))\n",
    "    \n",
    "    # messages.append({\"role\": \"system\", \"content\": WORDNET_PROMPT})\n",
    "    out_synsets = []\n",
    "    # out_synsets.append(answer[\"content\"])\n",
    "    try:\n",
    "        for elem in ET.fromstring(answer['content']).findall('definition'):\n",
    "            prompt = f\"\"\"ID: {WORD_ID},\n",
    "                Word: {elem.find(\"word\").text},\n",
    "                Type: {elem.find(\"type\").text},\n",
    "                Meaning: {elem.find(\"meaning\").text},\n",
    "                Example: {elem.find(\"example\").text}\"\"\"\n",
    "                \n",
    "            out_str = f\"\"\"<synset id=\"{WORD_ID}\" word=\"{elem.find(\"word\").text}\" type=\"{elem.find(\"type\").text}\">\n",
    "                <generated>\n",
    "                <meaning>{elem.find(\"meaning\").text}</meaning>\n",
    "                <example>{elem.find(\"example\").text}</example>\n",
    "                \"\"\"\n",
    "            for relation in relations:\n",
    "                temp_list = messages.copy()\n",
    "                temp_list.append({\"role\": \"system\", \"content\": get_prompt_str_xml(relation)})\n",
    "                \n",
    "                temp_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "                answer = gen_from_prompt(temp_list, {'synset' :['meaning', 'example', 'synonyms']})\n",
    "                out_str += answer[\"content\"]\n",
    "                # out_synsets.append(answer[\"content\"])\n",
    "            if is_est:\n",
    "                wn_synset = find_wordnet_synset_est(elem.find(\"word\").text, elem.find(\"meaning\").text)\n",
    "            else:\n",
    "                wn_synset = find_wordnet_synset(elem.find(\"word\").text, elem.find(\"meaning\").text)\n",
    "            \n",
    "            out_str += f\"\"\"\n",
    "            </generated>\n",
    "            <actual>\n",
    "            <wn_name>{wn_synset.name if is_est else wn_synset.name()}</wn_name>\n",
    "            <meaning>{wn_synset.definition if is_est else wn_synset.definition()}</meaning>\"\"\"\n",
    "            for relation in relations:\n",
    "                try:\n",
    "                    synset = get_word_synset_est(wn_synset, relation) if is_est else get_word_synset(wn_synset, relation)\n",
    "                except:\n",
    "                    continue\n",
    "                out_str += f\"\"\"\n",
    "                <{relation}s>\n",
    "                {synset}\n",
    "                </{relation}s>\"\"\"\n",
    "            out_str += f\"\"\"\n",
    "            </actual>\n",
    "            </synset>\n",
    "            \"\"\"\n",
    "            # out_str += f\"\"\"\n",
    "            # </generated>\n",
    "            # </synset>\"\"\"\n",
    "            out_synsets.append(out_str)\n",
    "            WORD_ID += 1\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(\"DEBUG: answer:\", elem.find(\"word\").text, elem.find(\"meaning\").text)\n",
    "        return answer\n",
    "    return out_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keel\n",
      "tee\n",
      "lill\n",
      "pall\n",
      "tihane\n"
     ]
    }
   ],
   "source": [
    "# input_file = 'test.txt'\n",
    "input_file = 'test_est.txt'\n",
    "cur_time = datetime.datetime.now()\n",
    "if input_file is not None:\n",
    "    with open(input_file, 'r') as in_fp, open(f'{cur_time}_broken.xml', 'w') as broken_fp:\n",
    "        root = ET.Element('synsets')\n",
    "        for line in in_fp.readlines():\n",
    "            print(line.strip())\n",
    "            synsets = main_xml(prompt=line.strip(), is_est=True)\n",
    "            for synset in synsets:\n",
    "                try:\n",
    "                    root.append(ET.fromstring(synset))\n",
    "                    # ET.ElementTree(root).write(f'{cur_time}_output.xml')\n",
    "                except ET.ParseError:\n",
    "                    broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                    if synset is not None:\n",
    "                        broken_fp.write(synset + \"\\n\")\n",
    "        ET.ElementTree(root).write(f'{cur_time}_output.xml', encoding=\"UTF-8\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_file='test.txt', is_est=False, cur_time=None):\n",
    "    WORD_ID = 1\n",
    "    if cur_time is None:\n",
    "        cur_time = datetime.datetime.now()\n",
    "    with open(input_file, 'r') as in_fp, open(f'{cur_time}_broken.xml', 'w') as broken_fp:\n",
    "        root = ET.Element('synsets')\n",
    "        for line in in_fp.readlines():\n",
    "            print(line.strip())\n",
    "            messages = [\n",
    "                # {\"role\": \"system\", \"content\":  get_initial_prompt(is_est)},\n",
    "                {\"role\": \"system\", \"content\":  get_initial_prompt_est()},\n",
    "                {\"role\": \"user\", \"content\": line.strip()},\n",
    "            ]\n",
    "            check = False\n",
    "            for _ in range(3):\n",
    "                answer = gen_from_prompt(messages, None)\n",
    "                if answer is not None and 'content' in answer:\n",
    "                    answer_ls = [el.split(':')[-1].strip() for el in answer['content'].split('\\n') if len(el) > 0]\n",
    "                    if len(answer_ls) % 4 == 0:\n",
    "                        check = True\n",
    "                        break\n",
    "            if not check:\n",
    "                broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                continue\n",
    "            print(\"GEN: meanings, size:\", len(answer_ls)//4, answer_ls)\n",
    "            messages.append(dict(answer))\n",
    "            # answer_ls = answer['content'].split('\\n')\n",
    "            for i in range(len(answer_ls)//4):\n",
    "                print(f\"GEN: {i+1}th word: {answer_ls[i*4]}\")\n",
    "                xml_str = f\"\"\"<synset id=\"{WORD_ID}\" word=\"{answer_ls[i*4]}\" type=\"{answer_ls[i*4+1]}\">\n",
    "                    <generated>\n",
    "                    <meaning>{answer_ls[i*4+2]}</meaning>\n",
    "                    <example>{answer_ls[i*4+3]}</example>\n",
    "                    \"\"\"\n",
    "                gen_rel_dict = {}\n",
    "                for relation in relations:\n",
    "                    temp_list = messages.copy()\n",
    "                    # temp_list.append({\"role\": \"system\", \"content\": get_prompt_str(relation)})\n",
    "                    temp_list.append({\"role\": \"system\", \"content\": get_prompt_str_est(relation)})\n",
    "                    prompt = f\"\"\"Word: {answer_ls[i*4]},\n",
    "                        Type: {answer_ls[i*4+1]},\n",
    "                        Meaning: {answer_ls[i*4+2]},\n",
    "                        Example: {answer_ls[i*4+3]}\"\"\"\n",
    "                    temp_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "                    check = False\n",
    "                    for _ in range(3):\n",
    "                        rel_answer = gen_from_prompt(temp_list, None)\n",
    "                        if rel_answer is not None and 'content' in  rel_answer:\n",
    "                            check = True\n",
    "                            break\n",
    "                    if not check:\n",
    "                        gen_rel_dict[relation] = []\n",
    "                        continue\n",
    "                    rel_answer_ls = rel_answer['content'].split('\\n')\n",
    "                    rel_answer_ls = [rel.strip().lower().replace(' ', '_') for rel in rel_answer_ls]\n",
    "                    gen_rel_dict[relation] = rel_answer_ls\n",
    "                    xml_str += f\"\"\"<{relation}s>{rel_answer_ls}</{relation}s>\"\"\"\n",
    "                xml_str += \"</generated>\"\n",
    "                if is_est:\n",
    "                    wn_synset = find_wordnet_synset_est(answer_ls[i*4], answer_ls[i*4+2])\n",
    "                else:\n",
    "                    wn_synset = find_wordnet_synset(answer_ls[i*4], answer_ls[i*4+2])\n",
    "                actual_rel_dict = dict.fromkeys(relations, [])\n",
    "                if wn_synset is None:\n",
    "                    xml_str += f\"\"\"\n",
    "                    <actual>NONE</actual>\n",
    "                    <stats>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    xml_str += f\"\"\"\n",
    "                    <actual>\n",
    "                    <wn_name>{wn_synset.name if is_est else wn_synset.name()}</wn_name>\n",
    "                    <meaning>{wn_synset.definition if is_est else wn_synset.definition()}</meaning>\"\"\"\n",
    "                    for relation in relations:\n",
    "                        try:\n",
    "                            synset = get_word_synset_est(wn_synset, relation) if is_est else get_word_synset(wn_synset, relation)\n",
    "                        except:\n",
    "                            synset = []\n",
    "                        if is_est:\n",
    "                            if relation == 'synonym':\n",
    "                                synset = [s.lower() for s in synset]\n",
    "                            else:\n",
    "                                synset = [s.name.lower().split('.')[0] for s in synset]\n",
    "                        else:\n",
    "                            synset = [s.name().lower().split('.')[0] for s in synset]\n",
    "                        actual_rel_dict[relation] = synset\n",
    "                        \n",
    "                        xml_str += f\"\"\"\n",
    "                        <{relation}s>\n",
    "                        {synset}\n",
    "                        </{relation}s>\"\"\"\n",
    "                    xml_str += f\"\"\"\n",
    "                    </actual>\n",
    "                    <stats>\n",
    "                    \"\"\"\n",
    "                total_gen = 0\n",
    "                total_actual = 0\n",
    "                total_overlapping = 0\n",
    "                total_over_generated = 0\n",
    "                total_under_generated = 0\n",
    "                for relation in relations:\n",
    "                    gen_rel_set = set(gen_rel_dict[relation])\n",
    "                    actual_rel_set = set(actual_rel_dict[relation])\n",
    "                    cur_total_gen = len(gen_rel_dict[relation])\n",
    "                    cur_total_actual = len(actual_rel_dict[relation])\n",
    "                    cur_total_overlapping = len(gen_rel_set.intersection(actual_rel_set))\n",
    "                    cur_total_over_generated = len(gen_rel_set.difference(actual_rel_set))\n",
    "                    cur_total_under_generated = len(actual_rel_set.difference(gen_rel_set))\n",
    "                    total_actual += cur_total_actual\n",
    "                    total_gen += cur_total_gen\n",
    "                    total_overlapping += cur_total_overlapping\n",
    "                    total_over_generated += cur_total_over_generated\n",
    "                    total_under_generated += cur_total_under_generated\n",
    "                    xml_str += f\"\"\"\n",
    "                    <{relation}>\n",
    "                    <generated_size>{cur_total_gen}</generated_size>\n",
    "                    <actual_size>{cur_total_actual}</actual_size>\n",
    "                    <overlapping>{cur_total_overlapping}</overlapping>\n",
    "                    <over_generated>{cur_total_over_generated}</over_generated>\n",
    "                    <under_generated>{cur_total_under_generated}</under_generated>\n",
    "                    </{relation}>\"\"\"  \n",
    "                xml_str += f\"\"\"\n",
    "                <total>\n",
    "                <generated_size>{total_gen}</generated_size>\n",
    "                <actual_size>{total_actual}</actual_size>\n",
    "                <overlapping>{total_overlapping}</overlapping>\n",
    "                <over_generated>{total_over_generated}</over_generated>\n",
    "                <under_generated>{total_under_generated}</under_generated>\n",
    "                </total>\n",
    "                \"\"\"   \n",
    "                xml_str += f\"\"\"\n",
    "                </stats>\n",
    "                </synset>\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    root.append(ET.fromstring(xml_str))\n",
    "                except ET.ParseError:\n",
    "                    broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                    broken_fp.write(xml_str + \"\\n\")\n",
    "                WORD_ID += 1\n",
    "            \n",
    "            ET.ElementTree(root).write(f'{cur_time}_output.xml', encoding=\"UTF-8\")\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diseased\n",
      "\n",
      "Tokens used:  243 prompt + 70 completion = 313 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0045\n",
      "\n",
      "GEN: meanings, size: 1 ['diseased', 'omadussõna', 'Haigustest mõjutatud või haige; mittetervislik', 'Arstid uurisid diseased organit, et mõista haiguse ulatust.']\n",
      "GEN: 1th word: diseased\n",
      "\n",
      "Tokens used:  521 prompt + 27 completion = 548 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0060\n",
      "\n",
      "\n",
      "Tokens used:  529 prompt + 40 completion = 569 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0065\n",
      "\n",
      "\n",
      "Tokens used:  525 prompt + 0 completion = 525 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0053\n",
      "\n",
      "\n",
      "Tokens used:  525 prompt + 7 completion = 532 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0055\n",
      "\n",
      "hefa\n",
      "\n",
      "Tokens used:  242 prompt + 56 completion = 298 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0041\n",
      "\n",
      "GEN: meanings, size: 1 ['hefa', 'nimisõna', 'heebrea kirjanduses esinev naisnime vorm.', 'Ta nimetas oma tütart nimega Hefa.']\n",
      "GEN: 1th word: hefa\n",
      "\n",
      "Tokens used:  492 prompt + 0 completion = 492 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0049\n",
      "\n",
      "\n",
      "Tokens used:  500 prompt + 0 completion = 500 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0050\n",
      "\n",
      "\n",
      "Tokens used:  496 prompt + 0 completion = 496 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0050\n",
      "\n",
      "\n",
      "Tokens used:  496 prompt + 0 completion = 496 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0050\n",
      "\n",
      "marianas\n",
      "\n",
      "Tokens used:  243 prompt + 120 completion = 363 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0060\n",
      "\n",
      "GEN: meanings, size: 1 ['Marianas', 'nimisõna', 'Vaikse ookeani lääneosas asuv poolsaarte rühm, mis kuulub suuresti Ameerika Ühendriikidele ja millele annab nime selle läheduses asuv Maarianide süvik.', 'Maarianid koosnevad 15 saarest, mida tuntakse kui Põhja-Maarianide föderaalset osariiki.']\n",
      "GEN: 1th word: Marianas\n",
      "\n",
      "Tokens used:  621 prompt + 0 completion = 621 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0062\n",
      "\n",
      "\n",
      "Tokens used:  629 prompt + 0 completion = 629 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0063\n",
      "\n",
      "\n",
      "Tokens used:  625 prompt + 0 completion = 625 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0063\n",
      "\n",
      "\n",
      "Tokens used:  625 prompt + 0 completion = 625 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0063\n",
      "\n",
      "yore\n",
      "\n",
      "Tokens used:  242 prompt + 57 completion = 299 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0041\n",
      "\n",
      "GEN: meanings, size: 1 ['Yore', 'Määrsõna', 'Aeg ammuse mineviku tähenduses.', 'In days of yore, knights would joust for the honor of a lady.']\n",
      "GEN: 1th word: Yore\n",
      "\n",
      "Tokens used:  494 prompt + 28 completion = 522 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0058\n",
      "\n",
      "\n",
      "Tokens used:  502 prompt + 0 completion = 502 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0050\n",
      "\n",
      "\n",
      "Tokens used:  498 prompt + 0 completion = 498 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0050\n",
      "\n",
      "\n",
      "Tokens used:  498 prompt + 0 completion = 498 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0050\n",
      "\n",
      "roll\n",
      "\n",
      "Tokens used:  241 prompt + 480 completion = 721 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0168\n",
      "\n",
      "GEN: meanings, size: 7 ['roll', 'nimisõna', 'Professionaalne tegevus, mida keegi reaalses elus või teatrietendusel mängib.', 'Näitleja esitas oma rolli suurepäraselt.', 'roll', 'nimisõna', 'Rullis või silindriliselt kokku keeratud objekti või materjali tükk.', 'Palun ulata mulle tualettpaberi rull.', 'roll', 'nimisõna', 'Kirjalik dokument, mis on kokku rullitud, eriti pikslina.', 'Tal on kodus tõeliselt iidse rolli koopia.', 'roll', 'nimisõna', 'Leiba või saiakorv, eriti väike või ümmargune.', 'Ma panin võileiva asemel oma võileivatäidise saiaviilu asemel kukli peale.', 'roll', 'tegusõna', 'Liikuda või põhjustada objekti liikumist küljelt küljele, eriti sujuvalt ja järjepidevalt.', 'Ta veeretas palli mööda maja seina äärde.', 'roll', 'tegusõna', 'Libiseda ja pöörduda, eriti ümber oma telje.', 'Laev hakkas tormi tõttu rulluma.', 'roll', 'tegusõna', 'Keerata midagi lameda ja ümmarguse või silindrikujulise kujuni.', 'Ma rullisin plakati kokku ja panin selle kotti.']\n",
      "GEN: 1th word: roll\n",
      "\n",
      "Tokens used:  930 prompt + 39 completion = 969 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0105\n",
      "\n",
      "\n",
      "Tokens used:  938 prompt + 272 completion = 1,210 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0175\n",
      "\n",
      "\n",
      "Tokens used:  934 prompt + 144 completion = 1,078 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0137\n",
      "\n",
      "\n",
      "Tokens used:  934 prompt + 21 completion = 955 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0100\n",
      "\n",
      "GEN: 2th word: roll\n",
      "\n",
      "Tokens used:  925 prompt + 18 completion = 943 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0098\n",
      "\n",
      "\n",
      "Tokens used:  933 prompt + 42 completion = 975 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0106\n",
      "\n",
      "\n",
      "Tokens used:  929 prompt + 4 completion = 933 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0094\n",
      "\n",
      "\n",
      "Tokens used:  929 prompt + 0 completion = 929 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0093\n",
      "\n",
      "GEN: 3th word: roll\n",
      "\n",
      "Tokens used:  919 prompt + 19 completion = 938 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0098\n",
      "\n",
      "\n",
      "Tokens used:  927 prompt + 111 completion = 1,038 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0126\n",
      "\n",
      "\n",
      "Tokens used:  923 prompt + 14 completion = 937 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0097\n",
      "\n",
      "\n",
      "Tokens used:  923 prompt + 21 completion = 944 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0099\n",
      "\n",
      "GEN: 4th word: roll\n",
      "\n",
      "Tokens used:  933 prompt + 807 completion = 1,740 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0335\n",
      "\n",
      "\n",
      "Tokens used:  941 prompt + 19 completion = 960 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0100\n",
      "\n",
      "\n",
      "Tokens used:  937 prompt + 108 completion = 1,045 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0126\n",
      "\n",
      "\n",
      "Tokens used:  937 prompt + 0 completion = 937 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0094\n",
      "\n",
      "GEN: 5th word: roll\n",
      "\n",
      "Tokens used:  942 prompt + 25 completion = 967 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0102\n",
      "\n",
      "\n",
      "Tokens used:  950 prompt + 27 completion = 977 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0103\n",
      "\n",
      "\n",
      "Tokens used:  946 prompt + 0 completion = 946 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0095\n",
      "\n",
      "\n",
      "Tokens used:  946 prompt + 10 completion = 956 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0098\n",
      "\n",
      "GEN: 6th word: roll\n",
      "\n",
      "Tokens used:  919 prompt + 37 completion = 956 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0103\n",
      "\n",
      "\n",
      "Tokens used:  927 prompt + 278 completion = 1,205 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0176\n",
      "\n",
      "\n",
      "Tokens used:  923 prompt + 0 completion = 923 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0092\n",
      "\n",
      "\n",
      "Tokens used:  923 prompt + 14 completion = 937 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0097\n",
      "\n",
      "GEN: 7th word: roll\n",
      "\n",
      "Tokens used:  925 prompt + 17 completion = 942 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0098\n",
      "\n",
      "\n",
      "Tokens used:  933 prompt + 622 completion = 1,555 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0280\n",
      "\n",
      "\n",
      "Tokens used:  929 prompt + 0 completion = 929 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0093\n",
      "\n",
      "\n",
      "Tokens used:  929 prompt + 11 completion = 940 tokens\n",
      "Total cost for gpt-4-1106-preview: $0.0096\n",
      "\n"
     ]
    }
   ],
   "source": [
    "is_test = False\n",
    "cur_time = datetime.datetime.now()\n",
    "file_name = f'{cur_time}_random_words.txt'\n",
    "if not is_test:\n",
    "    rand_lines_nr = 5\n",
    "    with open('words.txt') as fp:\n",
    "    # with open('lemmad.txt') as fp:\n",
    "        rand_lines = random.sample(list(fp), rand_lines_nr)\n",
    "    with open(file_name, 'w') as fp:\n",
    "        fp.writelines(rand_lines)\n",
    "r = main(input_file=file_name, cur_time=cur_time, is_est=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46566\n"
     ]
    }
   ],
   "source": [
    "print(total_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(input_file):\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        # Get generated and actual synsets and compare relations by calculating the overlap\n",
    "        for synset in root.findall('synset'):\n",
    "            generated = synset.find('generated')\n",
    "            actual = synset.find('actual')\n",
    "            print(f\"Word: {synset.get('word')}, Type: {synset.get('type')}\")\n",
    "            print(\n",
    "                f\"Generated: {generated.find('meaning').text}, Actual: {actual.find('meaning').text}\")\n",
    "            for relation in relations:\n",
    "                gen_rel = generated.find(f\"{relation}s\")\n",
    "                act_rel = actual.find(f\"{relation}s\")\n",
    "                if gen_rel is None and act_rel is None:\n",
    "                    continue\n",
    "                if gen_rel is None or act_rel is None:\n",
    "                    print(\n",
    "                        f\"Relation: {relation}, Gen: {gen_rel}, Act: {act_rel}\")\n",
    "                    continue\n",
    "                gen_rel = set(gen_rel.text.split())\n",
    "                act_rel = set(act_rel.text.split())\n",
    "                print(f\"Relation: {relation}, Gen: {gen_rel}, Act: {act_rel}\")\n",
    "                print(\n",
    "                    f\"Overlap: {len(gen_rel.intersection(act_rel))}, Gen: {len(gen_rel)}, Act: {len(act_rel)}\")\n",
    "\n",
    "\n",
    "def count_total_stats(input_file):\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        total_gen = 0\n",
    "        total_actual = 0\n",
    "        total_overlapping = 0\n",
    "        total_over_generated = 0\n",
    "        total_under_generated = 0\n",
    "        for synset in root.findall('synset'):\n",
    "            stats = synset.find('stats')\n",
    "            total_stats = stats.find('total')\n",
    "            total_gen += int(total_stats.find('generated_size').text)\n",
    "            total_actual += int(total_stats.find('actual_size').text)\n",
    "            total_overlapping += int(total_stats.find('overlapping').text)\n",
    "            total_over_generated += int(total_stats.find('over_generated').text)\n",
    "            total_under_generated += int(\n",
    "                total_stats.find('under_generated').text)\n",
    "        print(f\"Total: Gen: {total_gen}, Act: {total_actual}, Overlap: {total_overlapping}, Over Gen: {total_over_generated}, Under Gen: {total_under_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Gen: 346, Act: 27, Overlap: 9, Over Gen: 337, Under Gen: 18\n"
     ]
    }
   ],
   "source": [
    "count_total_stats(f'{cur_time}_output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_synset(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas()\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms()\n",
    "        case 'meronym':\n",
    "            return synset.part_meronyms()\n",
    "        case 'antonym':\n",
    "            return synset.lemmas()[0].antonyms()\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms()\n",
    "        case 'holonyms':\n",
    "            return synset.member_holonyms()\n",
    "        case 'pertainyms':\n",
    "            return synset.pertainyms()\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")\n",
    "\n",
    "def get_word_synset_est(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms\n",
    "        case 'meronym':\n",
    "            return synset.meronyms\n",
    "        case 'antonym':\n",
    "            return synset.get_related_synset('antonym')\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms\n",
    "        case 'holonyms':\n",
    "            return synset.holonyms\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('kapsas.n.01')\n",
      "kapsas.n.01\n",
      "piltl kulunud, narmendav, katkine raamat, kaustik, vihik vms (EKSS)\n",
      "['kapsas']\n",
      "[\"Synset('raamatukapsas.n.01')\"]\n",
      "\n",
      "Synset('kapsapea.n.01')\n",
      "kapsapea.n.01\n",
      "saadus kapsa taimest\n",
      "['kapsapea', 'kapsas']\n",
      "[\"Synset('punane kapsas.n.01')\", \"Synset('riivkapsas.n.01')\"]\n",
      "\n",
      "Synset('kapsas.n.03')\n",
      "kapsas.n.03\n",
      "ristõieline köögivilja- ja söödakultuur (hrl. Brassica oleracea) (EKSS)\n",
      "['kapsas', 'kapsataim']\n",
      "[\"Synset('lillkapsas.n.02')\", \"Synset('asparkapsas.n.01')\", \"Synset('peakapsas.n.02')\", \"Synset('lehtkapsas.n.01')\", \"Synset('brüsseli kapsas.n.02')\", \"Synset('käharkapsas.n.01')\", \"Synset('hiina kapsas.n.01')\", \"Synset('koolrabi.n.01')\"]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in estwn['kapsas']:\n",
    "    print(i)\n",
    "    print(i.name)\n",
    "    print(i.definition)\n",
    "    print(i.lemmas)\n",
    "    print(i.hyponyms)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = estwn['kapsas']\n",
    "temp[2].get_related_synset('synonym')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('current.n.01'),\n",
       " Synset('current.n.02'),\n",
       " Synset('stream.n.02'),\n",
       " Synset('current.a.01')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('current')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['current', 'stream']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('current')[1].lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('keel.n.01')\n"
     ]
    }
   ],
   "source": [
    "print(find_wordnet_synset_est('keel', 'Inimene kasutab suuõõnes asuvat keelt kõnelemiseks ja toidu maitse tundmiseks.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Synset('lill.n.01')\"]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estwn['lill']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('crane.n.04')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = find_wordnet_synset('crane', 'A large machine for moving heavy objects by suspending them from a beam.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crane']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Synset('cat.n.01')\n",
      "feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\n",
      "['cat', 'true_cat']\n",
      "cat.n.01\n",
      "[Lemma('cat.n.01.cat'), Lemma('cat.n.01.true_cat')]\n",
      "1\n",
      "Synset('guy.n.01')\n",
      "an informal term for a youth or man\n",
      "['guy', 'cat', 'hombre', 'bozo']\n",
      "guy.n.01\n",
      "[Lemma('guy.n.01.guy'), Lemma('guy.n.01.cat'), Lemma('guy.n.01.hombre'), Lemma('guy.n.01.bozo')]\n",
      "2\n",
      "Synset('cat.n.03')\n",
      "a spiteful woman gossip\n",
      "['cat']\n",
      "cat.n.03\n",
      "[Lemma('cat.n.03.cat')]\n",
      "3\n",
      "Synset('kat.n.01')\n",
      "the leaves of the shrub Catha edulis which are chewed like tobacco or used to make tea; has the effect of a euphoric stimulant\n",
      "['kat', 'khat', 'qat', 'quat', 'cat', 'Arabian_tea', 'African_tea']\n",
      "kat.n.01\n",
      "[Lemma('kat.n.01.kat'), Lemma('kat.n.01.khat'), Lemma('kat.n.01.qat'), Lemma('kat.n.01.quat'), Lemma('kat.n.01.cat'), Lemma('kat.n.01.Arabian_tea'), Lemma('kat.n.01.African_tea')]\n",
      "4\n",
      "Synset('cat-o'-nine-tails.n.01')\n",
      "a whip with nine knotted cords\n",
      "[\"cat-o'-nine-tails\", 'cat']\n",
      "cat-o'-nine-tails.n.01\n",
      "[Lemma('cat-o'-nine-tails.n.01.cat-o'-nine-tails'), Lemma('cat-o'-nine-tails.n.01.cat')]\n",
      "5\n",
      "Synset('caterpillar.n.02')\n",
      "a large tracked vehicle that is propelled by two endless metal belts; frequently used for moving earth in construction and farm work\n",
      "['Caterpillar', 'cat']\n",
      "caterpillar.n.02\n",
      "[Lemma('caterpillar.n.02.Caterpillar'), Lemma('caterpillar.n.02.cat')]\n",
      "6\n",
      "Synset('big_cat.n.01')\n",
      "any of several large cats typically able to roar and living in the wild\n",
      "['big_cat', 'cat']\n",
      "big_cat.n.01\n",
      "[Lemma('big_cat.n.01.big_cat'), Lemma('big_cat.n.01.cat')]\n",
      "7\n",
      "Synset('computerized_tomography.n.01')\n",
      "a method of examining body organs by scanning them with X rays and using a computer to construct a series of cross-sectional scans along a single axis\n",
      "['computerized_tomography', 'computed_tomography', 'CT', 'computerized_axial_tomography', 'computed_axial_tomography', 'CAT']\n",
      "computerized_tomography.n.01\n",
      "[Lemma('computerized_tomography.n.01.computerized_tomography'), Lemma('computerized_tomography.n.01.computed_tomography'), Lemma('computerized_tomography.n.01.CT'), Lemma('computerized_tomography.n.01.computerized_axial_tomography'), Lemma('computerized_tomography.n.01.computed_axial_tomography'), Lemma('computerized_tomography.n.01.CAT')]\n",
      "8\n",
      "Synset('cat.v.01')\n",
      "beat with a cat-o'-nine-tails\n",
      "['cat']\n",
      "cat.v.01\n",
      "[Lemma('cat.v.01.cat')]\n",
      "9\n",
      "Synset('vomit.v.01')\n",
      "eject the contents of the stomach through the mouth\n",
      "['vomit', 'vomit_up', 'purge', 'cast', 'sick', 'cat', 'be_sick', 'disgorge', 'regorge', 'retch', 'puke', 'barf', 'spew', 'spue', 'chuck', 'upchuck', 'honk', 'regurgitate', 'throw_up']\n",
      "vomit.v.01\n",
      "[Lemma('vomit.v.01.vomit'), Lemma('vomit.v.01.vomit_up'), Lemma('vomit.v.01.purge'), Lemma('vomit.v.01.cast'), Lemma('vomit.v.01.sick'), Lemma('vomit.v.01.cat'), Lemma('vomit.v.01.be_sick'), Lemma('vomit.v.01.disgorge'), Lemma('vomit.v.01.regorge'), Lemma('vomit.v.01.retch'), Lemma('vomit.v.01.puke'), Lemma('vomit.v.01.barf'), Lemma('vomit.v.01.spew'), Lemma('vomit.v.01.spue'), Lemma('vomit.v.01.chuck'), Lemma('vomit.v.01.upchuck'), Lemma('vomit.v.01.honk'), Lemma('vomit.v.01.regurgitate'), Lemma('vomit.v.01.throw_up')]\n"
     ]
    }
   ],
   "source": [
    "for i, s in enumerate(wn.synsets('cat')):\n",
    "    # if is_person(s.lemma_names()):\n",
    "    #     continue\n",
    "    print(i)\n",
    "    print(s)\n",
    "    print(s.definition())\n",
    "    print(s.lemma_names())\n",
    "    print(s.name())\n",
    "    print(s.lemmas())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Crane', 'Stephen_Crane'],\n",
       " ['Crane', 'Harold_Hart_Crane', 'Hart_Crane'],\n",
       " ['Crane', 'Grus'],\n",
       " [],\n",
       " [],\n",
       " ['stretch_out']]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synonyms('crane')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
