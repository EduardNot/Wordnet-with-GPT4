{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/erudi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Downloading resources index: 20.1kB [00:00, 8.06MB/s]\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import datetime\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from estnltk.wordnet import Wordnet as EstWordnet\n",
    "import estnltk as et\n",
    "nltk.download('wordnet')\n",
    "estwn = EstWordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"API_TYPE\")\n",
    "openai.api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_base = os.getenv(\"API_BASE\")\n",
    "openai.api_version = os.getenv(\"API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_ID = 1\n",
    "\n",
    "\n",
    "def get_initial_prompt_xml(is_est=False):\n",
    "    return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation. You will be given a word and you have to give all defenitions of the word and give an example.\n",
    "            {\"You will be given the word in Estonian. Meaning and example must be in Estonian.\" if is_est else \"\"}\n",
    "            The output must contain only XML formatted answer. The XML must look like this:\n",
    "            <definitions>\n",
    "                <definition>\n",
    "                    <word>[Given word]</word>\n",
    "                    <type>[adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]</type>\n",
    "                    <meaning>[Meaning of the word]</meaning>\n",
    "                    <example>[An example sentece with given word]</example>\n",
    "                </definition>\n",
    "            </definitions>\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_prompt(is_est=False):\n",
    "    return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation. \n",
    "            You will be given a word and you must give all defenitions of the word{\" exactly like are in the Python NLTK library English WordNet\" if True else \"\"}.\n",
    "            The output must contain only plain text and must contain given word, word type, meaing of the word and and example separeted by a new line. \n",
    "            An example of the output:\n",
    "            Word: [Given word]\n",
    "            Type: [adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]\n",
    "            Meaning: [Meaning of the word]\n",
    "            Example: [An example sentece with given word]\n",
    "            Do not include any other information. Each difinition must have word, type, meaning and example. Each difinition must have only one meaning and one example.\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_prompt_est():\n",
    "    return f\"\"\" Sa oled kõrgelt kvalifitseeritud keele mõistmise ja WordNeti genereerimise AI.\n",
    "                Sulle antakse sõna ja sa pead andma kõik selle sõna definitsioonid{\" täpselt nagu on Python EstNLTK teegi Eesti WordNetis\" if True else \"\"}.\n",
    "                Väljund peab sisaldama ainult tavalist teksti ja peab sisaldama, uue reaga eraldatud: antud sõna, sõna tüüpi, sõna tähendust ja näide. Väljundi näide:\n",
    "                Sõna: [Antud sõna]\n",
    "                Tüüp: [omadussõna/abiverb/sidesõna/määrsõna/asesõna/nimisõna/palind/tegusõna]\n",
    "                Tähendus: [Sõna tähendus]\n",
    "                Näide: [Näide lause antud sõnaga]\n",
    "                Ära lisa väljundisse muud informatsiooni. Iga definitsioon peab sisaldama antud sõna, tüüp, tähendust ja näidet. Iga definitsioon peab sisaldama ainult ühte tähendust ja näidet.\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str_xml(word):\n",
    "    return f\"\"\"Now you will be given the following fields: id, word, type, meaning and example. \n",
    "    You will have to give exact {word}s {\"that are in WordNet in Python NLTK library\" if True else \"\"}.\n",
    "    The output must contain only XML. Here is an example of what XML must look like:\n",
    "\n",
    "            <{word}s>\n",
    "                <{word}>[{word} of word 1]</{word}>\n",
    "                <{word}>[{word} of word 2]</{word}>\n",
    "                <{word}>[{word} of word 3]</{word}>\n",
    "                ...\n",
    "            </{word}s>\n",
    "\n",
    "    The XML is just an example, there can be more or less {word}s for each word. If there are no {word}s, just leave the {word}s tag empty.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str(word):\n",
    "    return f\"\"\"Now you will be given the following fields: word, type, meaning and example.\n",
    "    You will have to give exact {word}s{\" that are exactly like in the Python NLTK library English Wordnet\" if True else \"\"}. \n",
    "    The output must contain only plain text. Here is an example of what the text must look like:\n",
    "            [First {word}]\n",
    "            [Second {word}]\n",
    "            [Third {word}]\n",
    "            ...\n",
    "    If there are no {word}s, just leave the output empty.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str_est(word):\n",
    "    return f\"\"\"Nüüd antakse sulle järgmised väljad: sõna, tüüp, tähendus ja näide.\n",
    "    Sa pead andma täpsed {word}id{\", mis on täpselt nagu Python EstNLTK teegi Eesti Wordnetis\" if True else \"\"}.\n",
    "    Väljund peab sisaldama ainult tavalist teksti. Siin on näide, kuidas tekst peab välja nägema:\n",
    "            [Esimene {word}]\n",
    "            [Teine {word}]\n",
    "            [Kolmas {word}]\n",
    "            ...\n",
    "    Kui sõnu pole, jäta väljund tühjaks.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "relations = ['synonym', 'hyponym', 'meronym', 'antonym', 'hypernym', 'holonym']\n",
    "relations_est = {'synonym': 'sünonüüm', \n",
    "                 'hyponym': 'hüponüüm', \n",
    "                 'meronym': 'meronüüm', \n",
    "                 'antonym': 'antonüüm',\n",
    "                 'hypernym': 'hüperonüüm',\n",
    "                 'holonym': 'holonüüm'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price = 0\n",
    "def openai_api_calculate_cost(usage, model=\"gpt-4-1106-preview\"): # https://community.openai.com/t/how-to-calculate-the-cost-of-a-specific-request-made-to-the-web-api-and-its-reply-in-tokens/270878/15\n",
    "    global total_price\n",
    "    pricing = {\n",
    "        'gpt-3.5-turbo-1106': {\n",
    "            'prompt': 0.001,\n",
    "            'completion': 0.002,\n",
    "        },\n",
    "        'gpt-4-1106-preview': {\n",
    "            'prompt': 0.01,\n",
    "            'completion': 0.03,\n",
    "        },\n",
    "        'gpt-4': {\n",
    "            'prompt': 0.03,\n",
    "            'completion': 0.06,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model_pricing = pricing[model]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model specified\")\n",
    "\n",
    "    prompt_cost = usage.prompt_tokens * model_pricing['prompt'] / 1000\n",
    "    completion_cost = usage.completion_tokens * \\\n",
    "        model_pricing['completion'] / 1000\n",
    "\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    # round to 6 decimals\n",
    "    total_cost = round(total_cost, 6)\n",
    "\n",
    "    # print(\n",
    "    #     f\"\\nTokens used:  {usage.prompt_tokens:,} prompt + {usage.completion_tokens:,} completion = {usage.total_tokens:,} tokens\")\n",
    "    # print(f\"Total cost for {model}: ${total_cost:.4f}\\n\")\n",
    "    total_price += total_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def is_person(words):\n",
    "    words_checked = [part[0].isupper()\n",
    "                     for word in words for part in word.split('_')]\n",
    "    return all(words_checked)\n",
    "\n",
    "\n",
    "def remove_short_words(words):\n",
    "    return [word for word in words if len(word) > 2]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/53416780/how-to-convert-token-list-into-wordnet-lemma-list-using-nltk\n",
    "def convert_to_lemma(sentence):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            lemmas += [synset.lemmas()[0].name()\n",
    "                       for synset in wn.synsets(token)]\n",
    "        except:\n",
    "            lemmas += [token]\n",
    "    return set(lemmas)\n",
    "\n",
    "\n",
    "def convert_to_lemma_est(sentence_ls):\n",
    "    sentence = ' '.join(sentence_ls)\n",
    "    text = et.Text(sentence)\n",
    "    lemmas_layer = text.tag_layer().morph_analysis.lemma\n",
    "    lemmas = [word for lemmas_list in lemmas_layer for word in lemmas_list]\n",
    "    return set(lemmas)\n",
    "\n",
    "\n",
    "def find_wordnet_synset(word, definition):\n",
    "    overlap = 0\n",
    "    most_overlap_synset = None\n",
    "    parsed_def = remove_short_words(definition.lower().split())\n",
    "    parsed_def_lemma = convert_to_lemma(parsed_def)\n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        # if is_person(synset.lemma_names()):\n",
    "        #     continue\n",
    "        actual_def = remove_short_words(synset.definition().lower().split())\n",
    "        actual_def_lemma = convert_to_lemma(actual_def)\n",
    "        overlap_temp = len(set(actual_def).intersection(set(parsed_def)))\n",
    "        overlap_temp_lemma = len(\n",
    "            actual_def_lemma.intersection(parsed_def_lemma))\n",
    "        overlap_temp += overlap_temp_lemma\n",
    "        if overlap_temp > overlap:\n",
    "            overlap = overlap_temp\n",
    "            most_overlap_synset = synset\n",
    "    return most_overlap_synset\n",
    "\n",
    "\n",
    "def get_word_synset(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas()\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms()\n",
    "        case 'meronym':\n",
    "            return synset.part_meronyms()\n",
    "        case 'antonym':\n",
    "            return synset.lemmas()[0].antonyms()\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms()\n",
    "        case 'holonym':\n",
    "            return synset.member_holonyms()\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")\n",
    "\n",
    "\n",
    "def find_wordnet_synset_est(word, definition):\n",
    "    overlap = -1\n",
    "    most_overlap_synset = None\n",
    "    parsed_def = remove_short_words(definition.lower().split())\n",
    "    parsed_def_lemma = convert_to_lemma_est(parsed_def)\n",
    "    for i, synset in enumerate(estwn[word]):\n",
    "        actual_def = remove_short_words(synset.definition.lower().split())\n",
    "        actual_def_lemma = convert_to_lemma_est(actual_def)\n",
    "        overlap_temp = len(set(actual_def).intersection(set(parsed_def)))\n",
    "        overlap_temp_lemma = len(\n",
    "            actual_def_lemma.intersection(parsed_def_lemma))\n",
    "        overlap_temp += overlap_temp_lemma\n",
    "        if overlap_temp > overlap:\n",
    "            overlap = overlap_temp\n",
    "            most_overlap_synset = synset\n",
    "    return most_overlap_synset\n",
    "\n",
    "\n",
    "def get_word_synset_est(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms\n",
    "        case 'meronym':\n",
    "            return synset.meronyms\n",
    "        case 'antonym':\n",
    "            return synset.get_related_synset('antonym')\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms\n",
    "        case 'holonym':\n",
    "            return synset.holonyms\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_from_prompt(msg, tags):\n",
    "    completion = None\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(deployment_id=\"gec\", model=\"gpt-4-1106-preview\", messages=msg)\n",
    "        answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # print(completion)\n",
    "        # if check_XML_validity(answer) and check_tags_XML(answer, tags):\n",
    "        #     break\n",
    "\n",
    "    except openai.error.ServiceUnavailableError:\n",
    "        pass\n",
    "        # Happens sometimes, just asking again usually helps\n",
    "\n",
    "    except openai.error.APIError:\n",
    "        pass\n",
    "        # Haven't looked, why does it happen, asking again helps usually\n",
    "    except KeyError as e:\n",
    "        if completion[\"choices\"][0][\"finish_reason\"] == \"content_filter\":\n",
    "            pass\n",
    "            # Some filter, happens even when nothing is wrong with the input, asking again might help\n",
    "\n",
    "    except openai.error.InvalidRequestError:\n",
    "        pass\n",
    "        # Aslo something related to input text\n",
    "\n",
    "    except openai.error.RateLimitError:\n",
    "        time.sleep(3)\n",
    "        # The error message said, that it's better to wait three seconds and try again\n",
    "    if completion is not None:\n",
    "        openai_api_calculate_cost(completion[\"usage\"])\n",
    "    return completion[\"choices\"][0][\"message\"] if completion is not None else None\n",
    "\n",
    "def check_XML_validity(xml_str):\n",
    "    try:\n",
    "        ET.fromstring(xml_str)\n",
    "        return True\n",
    "    except ET.ParseError:\n",
    "        return False\n",
    "    \n",
    "def check_tags_XML(xml_str, tags):\n",
    "    try:\n",
    "        for k, v in tags.items():\n",
    "            for elem in ET.fromstring(xml_str).iter(k):\n",
    "                for tag in v:\n",
    "                    if elem.find(tag) is None:\n",
    "                        return False\n",
    "        return True\n",
    "    except ET.ParseError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_file='test.txt', is_est=False, cur_time=None):\n",
    "    global root\n",
    "    WORD_ID = 1\n",
    "    if cur_time is None:\n",
    "        cur_time = datetime.datetime.now()\n",
    "    with open(input_file, 'r') as in_fp, open(f'{cur_time}_broken.xml', 'w') as broken_fp, open(f'{cur_time}_log.txt', 'w') as log_fp:\n",
    "        root = ET.Element('synsets')\n",
    "        for i, line in enumerate(in_fp.readlines()):\n",
    "            print(i, line.strip())\n",
    "            messages = [\n",
    "                # {\"role\": \"system\", \"content\":  get_initial_prompt(is_est)},\n",
    "                {\"role\": \"system\", \"content\":  get_initial_prompt_est() if is_est else get_initial_prompt()},\n",
    "                {\"role\": \"user\", \"content\": line.strip()},\n",
    "            ]\n",
    "            check = False\n",
    "            for _ in range(3):\n",
    "                answer = gen_from_prompt(messages, None)\n",
    "                if answer is not None and 'content' in answer:\n",
    "                    answer_ls = [el.split(':')[-1].strip() for el in answer['content'].split('\\n') if len(el) > 0]\n",
    "                    if len(answer_ls) % 4 == 0:\n",
    "                        for l in range(0, len(answer_ls), 4):\n",
    "                            if answer_ls[l].strip().lower() != line.strip().lower():\n",
    "                                break\n",
    "                        else:\n",
    "                            check = True\n",
    "                            break\n",
    "            if not check:\n",
    "                broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                continue\n",
    "            log_fp.write(f\"WORD: {line.strip()}\\n\")\n",
    "            log_fp.write(answer['content'] + \"\\n\")\n",
    "            print(\"GEN: meanings, size:\", len(answer_ls)//4, answer_ls)\n",
    "            messages.append(dict(answer))\n",
    "            # answer_ls = answer['content'].split('\\n')\n",
    "            for i in range(len(answer_ls)//4):\n",
    "                print(f\"GEN: {i+1}th word: {answer_ls[i*4]}\")\n",
    "                xml_str = f\"\"\"<synset id=\"{WORD_ID}\" word=\"{answer_ls[i*4]}\" type=\"{answer_ls[i*4+1]}\">\n",
    "                    <generated>\n",
    "                    <meaning>{answer_ls[i*4+2]}</meaning>\n",
    "                    <example>{answer_ls[i*4+3]}</example>\n",
    "                    \"\"\"\n",
    "                gen_rel_dict = {}\n",
    "                for relation in relations:\n",
    "                    temp_list = messages.copy()\n",
    "                    # temp_list.append({\"role\": \"system\", \"content\": get_prompt_str(relation)})\n",
    "                    temp_list.append({\"role\": \"system\", \"content\": get_prompt_str_est(relations_est[relation]) if is_est else get_prompt_str(relation)})\n",
    "                    prompt = f\"\"\"Word: {answer_ls[i*4]},\n",
    "                        Type: {answer_ls[i*4+1]},\n",
    "                        Meaning: {answer_ls[i*4+2]},\n",
    "                        Example: {answer_ls[i*4+3]}\"\"\"\n",
    "                    temp_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "                    check = False\n",
    "                    for _ in range(3):\n",
    "                        rel_answer = gen_from_prompt(temp_list, None)\n",
    "                        if rel_answer is not None and 'content' in  rel_answer:\n",
    "                            check = True\n",
    "                            break\n",
    "                    if not check:\n",
    "                        gen_rel_dict[relation] = []\n",
    "                        continue\n",
    "                    log_fp.write(f\"RELATION: {relation}\\n\\{rel_answer['content']}\\n\")\n",
    "                    rel_answer_ls = rel_answer['content'].split('\\n')\n",
    "                    rel_answer_ls = [rel.strip().lower().replace(' ', '_') for rel in rel_answer_ls]\n",
    "                    gen_rel_dict[relation] = rel_answer_ls\n",
    "                    xml_str += f\"\"\"<{relation}s>{rel_answer_ls}</{relation}s>\"\"\"\n",
    "                xml_str += \"</generated>\"\n",
    "                if is_est:\n",
    "                    wn_synset = find_wordnet_synset_est(answer_ls[i*4], answer_ls[i*4+2])\n",
    "                else:\n",
    "                    wn_synset = find_wordnet_synset(answer_ls[i*4], answer_ls[i*4+2])\n",
    "                actual_rel_dict = dict.fromkeys(relations, [])\n",
    "                if wn_synset is None:\n",
    "                    xml_str += f\"\"\"\n",
    "                    <actual>NONE</actual>\n",
    "                    <stats>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    xml_str += f\"\"\"\n",
    "                    <actual>\n",
    "                    <wn_name>{wn_synset.name if is_est else wn_synset.name()}</wn_name>\n",
    "                    <meaning>{wn_synset.definition if is_est else wn_synset.definition()}</meaning>\"\"\"\n",
    "                    for relation in relations:\n",
    "                        try:\n",
    "                            synset = get_word_synset_est(wn_synset, relation) if is_est else get_word_synset(wn_synset, relation)\n",
    "                        except:\n",
    "                            synset = []\n",
    "                        if is_est:\n",
    "                            if relation == 'synonym':\n",
    "                                synset = [s.lower() for s in synset]\n",
    "                            else:\n",
    "                                synset = [s.name.lower().split('.')[0] for s in synset]\n",
    "                        else:\n",
    "                            synset = [s.name().lower().split('.')[0] for s in synset]\n",
    "                        actual_rel_dict[relation] = synset\n",
    "                        \n",
    "                        xml_str += f\"\"\"\n",
    "                        <{relation}s>\n",
    "                        {synset}\n",
    "                        </{relation}s>\"\"\"\n",
    "                    xml_str += f\"\"\"\n",
    "                    </actual>\n",
    "                    <stats>\n",
    "                    \"\"\"\n",
    "                total_gen = 0\n",
    "                total_actual = 0\n",
    "                total_overlapping = 0\n",
    "                total_over_generated = 0\n",
    "                total_under_generated = 0\n",
    "                for relation in relations:\n",
    "                    gen_rel_set = set(gen_rel_dict[relation])\n",
    "                    actual_rel_set = set(actual_rel_dict[relation])\n",
    "                    cur_total_gen = len(gen_rel_dict[relation])\n",
    "                    cur_total_actual = len(actual_rel_dict[relation])\n",
    "                    cur_total_overlapping = len(gen_rel_set.intersection(actual_rel_set))\n",
    "                    cur_total_over_generated = len(gen_rel_set.difference(actual_rel_set))\n",
    "                    cur_total_under_generated = len(actual_rel_set.difference(gen_rel_set))\n",
    "                    total_actual += cur_total_actual\n",
    "                    total_gen += cur_total_gen\n",
    "                    total_overlapping += cur_total_overlapping\n",
    "                    total_over_generated += cur_total_over_generated\n",
    "                    total_under_generated += cur_total_under_generated\n",
    "                    xml_str += f\"\"\"\n",
    "                    <{relation}>\n",
    "                    <generated_size>{cur_total_gen}</generated_size>\n",
    "                    <actual_size>{cur_total_actual}</actual_size>\n",
    "                    <overlapping>{cur_total_overlapping}</overlapping>\n",
    "                    <over_generated>{cur_total_over_generated}</over_generated>\n",
    "                    <under_generated>{cur_total_under_generated}</under_generated>\n",
    "                    </{relation}>\"\"\"  \n",
    "                xml_str += f\"\"\"\n",
    "                <total>\n",
    "                <generated_size>{total_gen}</generated_size>\n",
    "                <actual_size>{total_actual}</actual_size>\n",
    "                <overlapping>{total_overlapping}</overlapping>\n",
    "                <over_generated>{total_over_generated}</over_generated>\n",
    "                <under_generated>{total_under_generated}</under_generated>\n",
    "                </total>\n",
    "                \"\"\"   \n",
    "                xml_str += f\"\"\"\n",
    "                </stats>\n",
    "                </synset>\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    root.append(ET.fromstring(xml_str))\n",
    "                except ET.ParseError:\n",
    "                    broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                    broken_fp.write(xml_str + \"\\n\")\n",
    "                WORD_ID += 1\n",
    "            \n",
    "            ET.ElementTree(root).write(f'{cur_time}_output.xml', encoding=\"UTF-8\")\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 keel\n",
      "GEN: meanings, size: 5 ['keel', 'nimisõna', 'suus paiknev liikuv organ, millel on tähtis roll toidu maitsmisel ja neelamisel ning häälikute moodustamisel', 'Ta näitas oma roosat keelt.', 'keel', 'nimisõna', 'abstraktne süsteem, mida inimesed kasutavad helide ja kirjalike märkide abil mõtete väljendamiseks ja edasiandmiseks', 'Eesti keel on oma keerukuses võluv.', 'keel', 'nimisõna', 'instrumendi osa, mille abil selle toimimist reguleeritakse', 'Kitarri keel.', 'keel', 'nimisõna', 'keeleteaduslik tüpoloogia tanüümiks nimetamissüsteem.', 'Isurite keel kuulub soome-ugri keelkonda.', 'keel', 'nimisõna', 'Keeleteaduse mõiste, mille abil jaotatakse keeli nende struktuuri või päritolu järgi.', 'Soome keel on soome-ugri keel.']\n",
      "GEN: 1th word: keel\n",
      "GEN: 2th word: keel\n",
      "GEN: 3th word: keel\n",
      "GEN: 4th word: keel\n",
      "GEN: 5th word: keel\n",
      "1 kurg\n",
      "GEN: meanings, size: 4 ['kurg', 'nimisõna', 'inimese kaelasopa sees paiknev luu- ja lihaseline moodustis, mis on nikastamise korral inimese hääleaparaat ja võimaldab neelamist', 'Pärast pikka laulmist oli tal kurg kähisev.', 'kurg', 'nimisõna', 'suur veetüüpi lind, kellel on pikk kael ja jalad ning sirge terava otsaga nokk, kütk roosteehitus, eriti rännulind.', 'Me nägime, kuidas kureparv lõuna poole lendas.', 'kurg', 'nimisõna', 'Nõu (eriti pudeli) kitsas suue.', 'Tema voolas viski otse pudelikurgust kurku.', 'kurg', 'nimisõna', 'Aia- ja köögiviljakultuur.', 'Ta istutas mõned juurviljad aiasse, sealhulgas kurgid.']\n",
      "GEN: 1th word: kurg\n",
      "GEN: 2th word: kurg\n",
      "GEN: 3th word: kurg\n",
      "GEN: 4th word: kurg\n",
      "2 lill\n",
      "GEN: meanings, size: 3 ['lill', 'nimisõna', 'õistaim, eriti selline, mida kasvatatakse ilusate õite pärast.', 'Tal on aias palju erinevaid lilli.', 'lill', 'nimisõna', 'millegi sageli ilu sümboliseeriv osa, kaunistus.', 'Käsitööna valminud vaibal oli keskel suur lill.', 'lill', 'nimisõna', 'noor, tütarlaps või naine.', 'Sa oled tõeline lill!']\n",
      "GEN: 1th word: lill\n",
      "GEN: 2th word: lill\n",
      "GEN: 3th word: lill\n",
      "3 pall\n",
      "GEN: meanings, size: 2 ['pall', 'nimisõna', 'kujult ümar ese, eriti spordiala vahend.', 'Lapsed mängisid õues jalgpalli, tagudes suurt ümmargust palli.', 'pall', 'nimisõna', 'väline, eriti sädelev, joonistus- või värvilise mustriga kaunistus taimel.', 'Jõulupuu oksad olid kaunistatud värviliste pallidega.']\n",
      "GEN: 1th word: pall\n",
      "GEN: 2th word: pall\n",
      "4 tihane\n",
      "GEN: meanings, size: 1 ['tihane', 'nimisõna', 'Väikesi tihaslaste sugukonda kuuluvaid laululinde, kes on aktiivsed, sotsiaalsed ja hästi lennata suutvad.', 'Tihane piiksus rõõmsalt puu oksal.']\n",
      "GEN: 1th word: tihane\n",
      "5 kapsas\n",
      "GEN: meanings, size: 3 ['kapsas', 'nimisõna', 'korvõieliste sugukonda kuuluv taim, millel on suur ümmargune, tihti lamedavõitu juurmise sisuga pea', 'Kapsas on oluline köögivili.', 'kapsas', 'nimisõna', 'raha', 'Mul pole praegu piisavalt kapsast, et seda asja osta.', 'kapsas', 'nimisõna', 'segadus, sass, jama, keeruline olukord', 'Ta sattus oma valetamisega täielikku kapsasse.']\n",
      "GEN: 1th word: kapsas\n",
      "GEN: 2th word: kapsas\n",
      "GEN: 3th word: kapsas\n",
      "6 ülikool\n",
      "GEN: meanings, size: 1 ['Ülikool', 'Nimisõna', 'Kõrgharidust andev õppeasutus, kus õppetöö toimub teaduslikul alusel.', 'Tartu Ülikool on Eesti vanim ja suurim ülikool.']\n",
      "GEN: 1th word: Ülikool\n",
      "7 guugeldamine\n",
      "GEN: meanings, size: 1 ['guugeldamine', 'nimisõna', 'veebisaidil Google oleva otsingumootori kasutamine, et leida teavet (kellegi või millegi kohta).', 'Ta veetis terve päeva guugeldades infot oma esitluse jaoks.']\n",
      "GEN: 1th word: guugeldamine\n",
      "8 arvuti\n",
      "GEN: meanings, size: 5 ['arvuti', 'nimisõna', 'digitaaltehnika saavutuste põhjal loodud seade, mis on võimeline töötama kindla programmi järgi ning tegema teatavaid toiminguid ettenähtud tulemuse saavutamiseks; peaaju; infotehnoloogia', 'Arvuti käivitamine kestab umbes minuti.', 'arvuti', 'nimisõna', 'kõik ca arvutit, mida üks isik või organisatsioon omab ning mida ta või see saab tehnoloogia abil tööle panna nii, et tulemused vastavad kindlatele nõuetele', 'Valitsusel on eelarve, millega osta arvuteid.', 'arvuti', 'nimisõna', 'objekt või subjekt, mis loob, säilitab ja töötleb informatsiooni teatud viisil, mis võimaldab sellel teha kindlaid toiminguid ning saavutada ettenähtud tulemusi', 'Firma kontoris on palju arvuteid.', 'arvuti', 'nimisõna', 'disaineri töö, mis seisneb tehnoloogia ja informatsiooni kasutamises, et saavutada teatavaid tulemusi', 'Naine istub arvuti taga terve päeva, tegeledes graafilise disainiga.', 'arvuti', 'nimisõna', 'organisatsioon või isik, kes omab ja kasutab tehnoloogiat ja infotehnoloogiat ning on võimeline saavutama ettenähtud tulemusi', 'Arvuti on muutnud meie elu oluliselt.']\n",
      "GEN: 1th word: arvuti\n",
      "GEN: 2th word: arvuti\n",
      "GEN: 3th word: arvuti\n",
      "GEN: 4th word: arvuti\n",
      "GEN: 5th word: arvuti\n"
     ]
    }
   ],
   "source": [
    "is_test = True\n",
    "cur_time = datetime.datetime.now()\n",
    "file_name = f'{cur_time}_random_words.txt'\n",
    "if not is_test:\n",
    "    rand_lines_nr = 5\n",
    "    with open('words.txt') as fp:\n",
    "    # with open('lemmad.txt') as fp:\n",
    "        rand_lines = random.sample(list(fp), rand_lines_nr)\n",
    "    with open(file_name, 'w') as fp:\n",
    "        fp.writelines(rand_lines)\n",
    "    r = main(input_file=file_name, cur_time=cur_time, is_est=False)\n",
    "else:\n",
    "    r = main(input_file='test_est.txt', cur_time=cur_time, is_est=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2071799999999997\n"
     ]
    }
   ],
   "source": [
    "print(total_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(input_file):\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        # Get generated and actual synsets and compare relations by calculating the overlap\n",
    "        for synset in root.findall('synset'):\n",
    "            generated = synset.find('generated')\n",
    "            actual = synset.find('actual')\n",
    "            print(f\"Word: {synset.get('word')}, Type: {synset.get('type')}\")\n",
    "            print(\n",
    "                f\"Generated: {generated.find('meaning').text}, Actual: {actual.find('meaning').text}\")\n",
    "            for relation in relations:\n",
    "                gen_rel = generated.find(f\"{relation}s\")\n",
    "                act_rel = actual.find(f\"{relation}s\")\n",
    "                if gen_rel is None and act_rel is None:\n",
    "                    continue\n",
    "                if gen_rel is None or act_rel is None:\n",
    "                    print(\n",
    "                        f\"Relation: {relation}, Gen: {gen_rel}, Act: {act_rel}\")\n",
    "                    continue\n",
    "                gen_rel = set(gen_rel.text.split())\n",
    "                act_rel = set(act_rel.text.split())\n",
    "                print(f\"Relation: {relation}, Gen: {gen_rel}, Act: {act_rel}\")\n",
    "                print(\n",
    "                    f\"Overlap: {len(gen_rel.intersection(act_rel))}, Gen: {len(gen_rel)}, Act: {len(act_rel)}\")\n",
    "\n",
    "\n",
    "def count_total_stats(input_file):\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        total_gen = 0\n",
    "        total_actual = 0\n",
    "        total_overlapping = 0\n",
    "        total_over_generated = 0\n",
    "        total_under_generated = 0\n",
    "        for synset in root.findall('synset'):\n",
    "            if synset.find('actual').text == 'NONE':\n",
    "                continue\n",
    "            stats = synset.find('stats')\n",
    "            total_stats = stats.find('total')\n",
    "            total_gen += int(total_stats.find('generated_size').text)\n",
    "            total_actual += int(total_stats.find('actual_size').text)\n",
    "            total_overlapping += int(total_stats.find('overlapping').text)\n",
    "            total_over_generated += int(total_stats.find('over_generated').text)\n",
    "            total_under_generated += int(\n",
    "                total_stats.find('under_generated').text)\n",
    "        print(f\"Total: Gen: {total_gen}, Act: {total_actual}, Overlap: {total_overlapping}, Over Gen: {total_over_generated}, Under Gen: {total_under_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_meanings(input_file, is_est=False):\n",
    "    found_words = set()\n",
    "    actual_meanings = 0\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        total_meanings = 0\n",
    "        for synset in root.findall('synset'):\n",
    "            if synset.get('word') not in found_words:\n",
    "                found_words.add(synset.get('word'))\n",
    "                if is_est:\n",
    "                    actual_meanings += len(estwn[synset.get('word')])\n",
    "                else:\n",
    "                    actual_meanings += len(wn.synsets(synset.get('word')))\n",
    "            total_meanings += 1\n",
    "        print(f\"Total: {total_meanings}, Actual: {actual_meanings}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 36, Actual: 60\n",
      "Total: 20, Actual: 21\n",
      "Total: 35, Actual: 60\n",
      "Total: 25, Actual: 21\n"
     ]
    }
   ],
   "source": [
    "count_total_meanings('orig_prompt_eng.xml')\n",
    "count_total_meanings('orig_prompt_est.xml', is_est=True)\n",
    "count_total_meanings('new_prompt_eng.xml')\n",
    "count_total_meanings('new_prompt_est.xml', is_est=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
