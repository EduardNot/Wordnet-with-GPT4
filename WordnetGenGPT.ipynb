{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/erudi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "import datetime\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "from estnltk.wordnet import Wordnet as EstWordnet\n",
    "import estnltk as et\n",
    "nltk.download('wordnet')\n",
    "estwn = EstWordnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai.api_type = os.getenv(\"API_TYPE\")\n",
    "openai.api_key = os.getenv(\"API_KEY\")\n",
    "openai.api_base = os.getenv(\"API_BASE\")\n",
    "openai.api_version = os.getenv(\"API_VERSION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_ID = 1\n",
    "\n",
    "\n",
    "def get_initial_prompt_xml(is_est=False):\n",
    "    return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation. You will be given a word and you have to give all defenitions of the word and give an example.\n",
    "            {\"You will be given the word in Estonian. Meaning and example must be in Estonian.\" if is_est else \"\"}\n",
    "            The output must contain only XML formatted answer. The XML must look like this:\n",
    "            <definitions>\n",
    "                <definition>\n",
    "                    <word>[Given word]</word>\n",
    "                    <type>[adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]</type>\n",
    "                    <meaning>[Meaning of the word]</meaning>\n",
    "                    <example>[An example sentece with given word]</example>\n",
    "                </definition>\n",
    "            </definitions>\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_prompt(is_est=False):\n",
    "    return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation. \n",
    "            You will be given a word and you must give all defenitions of the word.\n",
    "            The output must contain only plain text and must contain given word, word type, meaing of the word and and example separeted by a new line. An example of the output:\n",
    "            Word: [Given word]\n",
    "            Type: [adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]\n",
    "            Meaning: [Meaning of the word]\n",
    "            Example: [An example sentece with given word]\n",
    "            Do not include any other information.\"\"\"\n",
    "    # return f\"\"\"You are a highly skilled AI trained in language comprehension and WordNet generation.\n",
    "    #         You will be given a word and you have to give all defenitions that are in Python NLTK library's WordNet. DO NOT INCLUDE people and places.\n",
    "    #         {\"You will be given the word in Estonian. Meaning and example must be in Estonian.\" if is_est else \"\"}\n",
    "    #         The output must contain only plain text and must contain given word, word type, meaing of the word and and example separeted by a new line. An example of the output:\n",
    "    #         [Given word]\n",
    "    #         [adjectives/adverbs/conjunctions/determiners/nouns/prepositions/pronouns/verbs]\n",
    "    #         [Meaning of the word]\n",
    "    #         [An example sentece with given word]\n",
    "    #         Do not include any other information in the output even \"Meaning\" and \"Example\" words.\"\"\"\n",
    "\n",
    "\n",
    "def get_initial_prompt_est():\n",
    "    return f\"\"\" Sa oled kõrgelt kvalifitseeritud keele mõistmise ja WordNeti genereerimise AI.\n",
    "                Sulle antakse sõna ja sa pead andma kõik selle sõna definitsioonid.\n",
    "                Väljund peab sisaldama ainult tavalist teksti ja peab sisaldama, uue reaga eraldatud: antud sõna, sõna tüüpi, sõna tähendust ja näide. Väljundi näide:\n",
    "                Sõna: [Antud sõna]\n",
    "                Tüüp: [omadussõna/abiverb/sidesõna/määrsõna/asesõna/nimisõna/palind/tegusõna]\n",
    "                Tähendus: [Sõna tähendus]\n",
    "                Näide: [Näide lause antud sõnaga]\n",
    "                Ära lisa väljundisse muud informatsiooni.\"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str_xml(word):\n",
    "    return f\"\"\"Now you will be given the following fields: id, word, type, meaning and example. \n",
    "    You will have to give exact {word}s {\"that are in WordNet in Python NLTK library\" if False else \"\"}.\n",
    "    The output must contain only XML. Here is an example of what XML must look like:\n",
    "\n",
    "            <{word}s>\n",
    "                <{word}>[{word} of word 1]</{word}>\n",
    "                <{word}>[{word} of word 2]</{word}>\n",
    "                <{word}>[{word} of word 3]</{word}>\n",
    "                ...\n",
    "            </{word}s>\n",
    "\n",
    "    The XML is just an example, there can be more or less {word}s for each word. If there are no {word}s, just leave the {word}s tag empty.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str(word):\n",
    "    return f\"\"\"Now you will be given the following fields: word, type, meaning and example.\n",
    "    You will have to give exact {word}s{\" that are in WordNet in Python NLTK library\" if False else \"\"}. \n",
    "    The output must contain only plain text. Here is an example of what the text must look like:\n",
    "            [First {word}]\n",
    "            [Second {word}]\n",
    "            [Third {word}]\n",
    "            ...\n",
    "    If there are no {word}s, just leave the output empty.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def get_prompt_str_est(word):\n",
    "    return f\"\"\"Nüüd antakse sulle järgmised väljad: sõna, tüüp, tähendus ja näide.\n",
    "    Sa pead andma täpsed {word}id, mis on Pythoni EstNLTK WordNetis.\n",
    "    Väljund peab sisaldama ainult tavalist teksti. Siin on näide, kuidas tekst peab välja nägema:\n",
    "            [Esimene {word}]\n",
    "            [Teine {word}]\n",
    "            [Kolmas {word}]\n",
    "            ...\n",
    "    Kui sõnu pole, jäta väljund tühjaks.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "relations = ['synonym', 'hyponym', 'meronym', 'antonym', 'hypernym', 'holonym']\n",
    "relations_est = ['sünonüüm', 'hüponüüm', 'meronüüm', 'antonüüm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_price = 0\n",
    "def openai_api_calculate_cost(usage, model=\"gpt-4-1106-preview\"): # https://community.openai.com/t/how-to-calculate-the-cost-of-a-specific-request-made-to-the-web-api-and-its-reply-in-tokens/270878/15\n",
    "    global total_price\n",
    "    pricing = {\n",
    "        'gpt-3.5-turbo-1106': {\n",
    "            'prompt': 0.001,\n",
    "            'completion': 0.002,\n",
    "        },\n",
    "        'gpt-4-1106-preview': {\n",
    "            'prompt': 0.01,\n",
    "            'completion': 0.03,\n",
    "        },\n",
    "        'gpt-4': {\n",
    "            'prompt': 0.03,\n",
    "            'completion': 0.06,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        model_pricing = pricing[model]\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid model specified\")\n",
    "\n",
    "    prompt_cost = usage.prompt_tokens * model_pricing['prompt'] / 1000\n",
    "    completion_cost = usage.completion_tokens * \\\n",
    "        model_pricing['completion'] / 1000\n",
    "\n",
    "    total_cost = prompt_cost + completion_cost\n",
    "    # round to 6 decimals\n",
    "    total_cost = round(total_cost, 6)\n",
    "\n",
    "    # print(\n",
    "    #     f\"\\nTokens used:  {usage.prompt_tokens:,} prompt + {usage.completion_tokens:,} completion = {usage.total_tokens:,} tokens\")\n",
    "    # print(f\"Total cost for {model}: ${total_cost:.4f}\\n\")\n",
    "    total_price += total_cost\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def is_person(words):\n",
    "    words_checked = [part[0].isupper()\n",
    "                     for word in words for part in word.split('_')]\n",
    "    return all(words_checked)\n",
    "\n",
    "\n",
    "def remove_short_words(words):\n",
    "    return [word for word in words if len(word) > 2]\n",
    "\n",
    "\n",
    "# https://stackoverflow.com/questions/53416780/how-to-convert-token-list-into-wordnet-lemma-list-using-nltk\n",
    "def convert_to_lemma(sentence):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in sentence]\n",
    "    lemmas = []\n",
    "    for token in text:\n",
    "        try:\n",
    "            lemmas += [synset.lemmas()[0].name()\n",
    "                       for synset in wn.synsets(token)]\n",
    "        except:\n",
    "            lemmas += [token]\n",
    "    return set(lemmas)\n",
    "\n",
    "\n",
    "def convert_to_lemma_est(sentence_ls):\n",
    "    sentence = ' '.join(sentence_ls)\n",
    "    text = et.Text(sentence)\n",
    "    lemmas_layer = text.tag_layer().morph_analysis.lemma\n",
    "    lemmas = [word for lemmas_list in lemmas_layer for word in lemmas_list]\n",
    "    return set(lemmas)\n",
    "\n",
    "\n",
    "def find_wordnet_synset(word, definition):\n",
    "    overlap = 0\n",
    "    most_overlap_synset = None\n",
    "    parsed_def = remove_short_words(definition.lower().split())\n",
    "    parsed_def_lemma = convert_to_lemma(parsed_def)\n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        if is_person(synset.lemma_names()):\n",
    "            continue\n",
    "        actual_def = remove_short_words(synset.definition().lower().split())\n",
    "        actual_def_lemma = convert_to_lemma(actual_def)\n",
    "        overlap_temp = len(set(actual_def).intersection(set(parsed_def)))\n",
    "        overlap_temp_lemma = len(\n",
    "            actual_def_lemma.intersection(parsed_def_lemma))\n",
    "        overlap_temp += overlap_temp_lemma\n",
    "        if overlap_temp > overlap:\n",
    "            overlap = overlap_temp\n",
    "            most_overlap_synset = synset\n",
    "    return most_overlap_synset\n",
    "\n",
    "\n",
    "def get_word_synset(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas()\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms()\n",
    "        case 'meronym':\n",
    "            return synset.part_meronyms()\n",
    "        case 'antonym':\n",
    "            return synset.lemmas()[0].antonyms()\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms()\n",
    "        case 'holonym':\n",
    "            return synset.member_holonyms()\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")\n",
    "\n",
    "\n",
    "def find_wordnet_synset_est(word, definition):\n",
    "    overlap = -1\n",
    "    most_overlap_synset = None\n",
    "    parsed_def = remove_short_words(definition.lower().split())\n",
    "    parsed_def_lemma = convert_to_lemma_est(parsed_def)\n",
    "    for i, synset in enumerate(estwn[word]):\n",
    "        actual_def = remove_short_words(synset.definition.lower().split())\n",
    "        actual_def_lemma = convert_to_lemma_est(actual_def)\n",
    "        overlap_temp = len(set(actual_def).intersection(set(parsed_def)))\n",
    "        overlap_temp_lemma = len(\n",
    "            actual_def_lemma.intersection(parsed_def_lemma))\n",
    "        overlap_temp += overlap_temp_lemma\n",
    "        if overlap_temp > overlap:\n",
    "            overlap = overlap_temp\n",
    "            most_overlap_synset = synset\n",
    "    return most_overlap_synset\n",
    "\n",
    "\n",
    "def get_word_synset_est(synset, syn_type):\n",
    "    match syn_type:\n",
    "        case 'synonym':\n",
    "            return synset.lemmas\n",
    "        case 'hyponym':\n",
    "            return synset.hyponyms\n",
    "        case 'meronym':\n",
    "            return synset.meronyms\n",
    "        case 'antonym':\n",
    "            return synset.get_related_synset('antonym')\n",
    "        case 'hypernym':\n",
    "            return synset.hypernyms\n",
    "        case 'holonym':\n",
    "            return synset.holonyms\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown syn_type: {syn_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_from_prompt(msg, tags):\n",
    "    completion = None\n",
    "    try:\n",
    "        completion = openai.ChatCompletion.create(deployment_id=\"gec\", model=\"gpt-4-1106-preview\", messages=msg)\n",
    "        answer = completion[\"choices\"][0][\"message\"][\"content\"]\n",
    "        # print(completion)\n",
    "        # if check_XML_validity(answer) and check_tags_XML(answer, tags):\n",
    "        #     break\n",
    "\n",
    "    except openai.error.ServiceUnavailableError:\n",
    "        pass\n",
    "        # Happens sometimes, just asking again usually helps\n",
    "\n",
    "    except openai.error.APIError:\n",
    "        pass\n",
    "        # Haven't looked, why does it happen, asking again helps usually\n",
    "    except KeyError as e:\n",
    "        if completion[\"choices\"][0][\"finish_reason\"] == \"content_filter\":\n",
    "            pass\n",
    "            # Some filter, happens even when nothing is wrong with the input, asking again might help\n",
    "\n",
    "    except openai.error.InvalidRequestError:\n",
    "        pass\n",
    "        # Aslo something related to input text\n",
    "\n",
    "    except openai.error.RateLimitError:\n",
    "        time.sleep(3)\n",
    "        # The error message said, that it's better to wait three seconds and try again\n",
    "    if completion is not None:\n",
    "        openai_api_calculate_cost(completion[\"usage\"])\n",
    "    return completion[\"choices\"][0][\"message\"] if completion is not None else None\n",
    "\n",
    "def check_XML_validity(xml_str):\n",
    "    try:\n",
    "        ET.fromstring(xml_str)\n",
    "        return True\n",
    "    except ET.ParseError:\n",
    "        return False\n",
    "    \n",
    "def check_tags_XML(xml_str, tags):\n",
    "    try:\n",
    "        for k, v in tags.items():\n",
    "            for elem in ET.fromstring(xml_str).iter(k):\n",
    "                for tag in v:\n",
    "                    if elem.find(tag) is None:\n",
    "                        return False\n",
    "        return True\n",
    "    except ET.ParseError:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(input_file='test.txt', is_est=False, cur_time=None):\n",
    "    WORD_ID = 1\n",
    "    if cur_time is None:\n",
    "        cur_time = datetime.datetime.now()\n",
    "    with open(input_file, 'r') as in_fp, open(f'{cur_time}_broken.xml', 'w') as broken_fp, open(f'{cur_time}_log.txt', 'w') as log_fp:\n",
    "        root = ET.Element('synsets')\n",
    "        for i, line in enumerate(in_fp.readlines()):\n",
    "            print(i, line.strip())\n",
    "            messages = [\n",
    "                # {\"role\": \"system\", \"content\":  get_initial_prompt(is_est)},\n",
    "                {\"role\": \"system\", \"content\":  get_initial_prompt_est() if is_est else get_initial_prompt()},\n",
    "                {\"role\": \"user\", \"content\": line.strip()},\n",
    "            ]\n",
    "            check = False\n",
    "            for _ in range(3):\n",
    "                answer = gen_from_prompt(messages, None)\n",
    "                if answer is not None and 'content' in answer:\n",
    "                    answer_ls = [el.split(':')[-1].strip() for el in answer['content'].split('\\n') if len(el) > 0]\n",
    "                    if len(answer_ls) % 4 == 0:\n",
    "                        for l in range(0, len(answer_ls), 4):\n",
    "                            if answer_ls[l].strip().lower() != line.strip().lower():\n",
    "                                break\n",
    "                        else:\n",
    "                            check = True\n",
    "                            break\n",
    "            if not check:\n",
    "                broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                continue\n",
    "            log_fp.write(f\"WORD: {line.strip()}\\n\")\n",
    "            log_fp.write(answer['content'] + \"\\n\")\n",
    "            print(\"GEN: meanings, size:\", len(answer_ls)//4, answer_ls)\n",
    "            messages.append(dict(answer))\n",
    "            # answer_ls = answer['content'].split('\\n')\n",
    "            for i in range(len(answer_ls)//4):\n",
    "                print(f\"GEN: {i+1}th word: {answer_ls[i*4]}\")\n",
    "                xml_str = f\"\"\"<synset id=\"{WORD_ID}\" word=\"{answer_ls[i*4]}\" type=\"{answer_ls[i*4+1]}\">\n",
    "                    <generated>\n",
    "                    <meaning>{answer_ls[i*4+2]}</meaning>\n",
    "                    <example>{answer_ls[i*4+3]}</example>\n",
    "                    \"\"\"\n",
    "                gen_rel_dict = {}\n",
    "                for relation in relations:\n",
    "                    temp_list = messages.copy()\n",
    "                    # temp_list.append({\"role\": \"system\", \"content\": get_prompt_str(relation)})\n",
    "                    temp_list.append({\"role\": \"system\", \"content\": get_prompt_str_est(relation) if is_est else get_prompt_str(relation)})\n",
    "                    prompt = f\"\"\"Word: {answer_ls[i*4]},\n",
    "                        Type: {answer_ls[i*4+1]},\n",
    "                        Meaning: {answer_ls[i*4+2]},\n",
    "                        Example: {answer_ls[i*4+3]}\"\"\"\n",
    "                    temp_list.append({\"role\": \"user\", \"content\": prompt})\n",
    "                    check = False\n",
    "                    for _ in range(3):\n",
    "                        rel_answer = gen_from_prompt(temp_list, None)\n",
    "                        if rel_answer is not None and 'content' in  rel_answer:\n",
    "                            check = True\n",
    "                            break\n",
    "                    if not check:\n",
    "                        gen_rel_dict[relation] = []\n",
    "                        continue\n",
    "                    log_fp.write(f\"RELATION: {relation}\\n\\{rel_answer['content']}\\n\")\n",
    "                    rel_answer_ls = rel_answer['content'].split('\\n')\n",
    "                    rel_answer_ls = [rel.strip().lower().replace(' ', '_') for rel in rel_answer_ls]\n",
    "                    gen_rel_dict[relation] = rel_answer_ls\n",
    "                    xml_str += f\"\"\"<{relation}s>{rel_answer_ls}</{relation}s>\"\"\"\n",
    "                xml_str += \"</generated>\"\n",
    "                if is_est:\n",
    "                    wn_synset = find_wordnet_synset_est(answer_ls[i*4], answer_ls[i*4+2])\n",
    "                else:\n",
    "                    wn_synset = find_wordnet_synset(answer_ls[i*4], answer_ls[i*4+2])\n",
    "                actual_rel_dict = dict.fromkeys(relations, [])\n",
    "                if wn_synset is None:\n",
    "                    xml_str += f\"\"\"\n",
    "                    <actual>NONE</actual>\n",
    "                    <stats>\n",
    "                    \"\"\"\n",
    "                else:\n",
    "                    xml_str += f\"\"\"\n",
    "                    <actual>\n",
    "                    <wn_name>{wn_synset.name if is_est else wn_synset.name()}</wn_name>\n",
    "                    <meaning>{wn_synset.definition if is_est else wn_synset.definition()}</meaning>\"\"\"\n",
    "                    for relation in relations:\n",
    "                        try:\n",
    "                            synset = get_word_synset_est(wn_synset, relation) if is_est else get_word_synset(wn_synset, relation)\n",
    "                        except:\n",
    "                            synset = []\n",
    "                        if is_est:\n",
    "                            if relation == 'synonym':\n",
    "                                synset = [s.lower() for s in synset]\n",
    "                            else:\n",
    "                                synset = [s.name.lower().split('.')[0] for s in synset]\n",
    "                        else:\n",
    "                            synset = [s.name().lower().split('.')[0] for s in synset]\n",
    "                        actual_rel_dict[relation] = synset\n",
    "                        \n",
    "                        xml_str += f\"\"\"\n",
    "                        <{relation}s>\n",
    "                        {synset}\n",
    "                        </{relation}s>\"\"\"\n",
    "                    xml_str += f\"\"\"\n",
    "                    </actual>\n",
    "                    <stats>\n",
    "                    \"\"\"\n",
    "                total_gen = 0\n",
    "                total_actual = 0\n",
    "                total_overlapping = 0\n",
    "                total_over_generated = 0\n",
    "                total_under_generated = 0\n",
    "                for relation in relations:\n",
    "                    gen_rel_set = set(gen_rel_dict[relation])\n",
    "                    actual_rel_set = set(actual_rel_dict[relation])\n",
    "                    cur_total_gen = len(gen_rel_dict[relation])\n",
    "                    cur_total_actual = len(actual_rel_dict[relation])\n",
    "                    cur_total_overlapping = len(gen_rel_set.intersection(actual_rel_set))\n",
    "                    cur_total_over_generated = len(gen_rel_set.difference(actual_rel_set))\n",
    "                    cur_total_under_generated = len(actual_rel_set.difference(gen_rel_set))\n",
    "                    total_actual += cur_total_actual\n",
    "                    total_gen += cur_total_gen\n",
    "                    total_overlapping += cur_total_overlapping\n",
    "                    total_over_generated += cur_total_over_generated\n",
    "                    total_under_generated += cur_total_under_generated\n",
    "                    xml_str += f\"\"\"\n",
    "                    <{relation}>\n",
    "                    <generated_size>{cur_total_gen}</generated_size>\n",
    "                    <actual_size>{cur_total_actual}</actual_size>\n",
    "                    <overlapping>{cur_total_overlapping}</overlapping>\n",
    "                    <over_generated>{cur_total_over_generated}</over_generated>\n",
    "                    <under_generated>{cur_total_under_generated}</under_generated>\n",
    "                    </{relation}>\"\"\"  \n",
    "                xml_str += f\"\"\"\n",
    "                <total>\n",
    "                <generated_size>{total_gen}</generated_size>\n",
    "                <actual_size>{total_actual}</actual_size>\n",
    "                <overlapping>{total_overlapping}</overlapping>\n",
    "                <over_generated>{total_over_generated}</over_generated>\n",
    "                <under_generated>{total_under_generated}</under_generated>\n",
    "                </total>\n",
    "                \"\"\"   \n",
    "                xml_str += f\"\"\"\n",
    "                </stats>\n",
    "                </synset>\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    root.append(ET.fromstring(xml_str))\n",
    "                except ET.ParseError:\n",
    "                    broken_fp.write('BROKEN WORD: ' + line.strip() + \"\\n\")\n",
    "                    broken_fp.write(xml_str + \"\\n\")\n",
    "                WORD_ID += 1\n",
    "            \n",
    "            ET.ElementTree(root).write(f'{cur_time}_output.xml', encoding=\"UTF-8\")\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 crane\n",
      "GEN: meanings, size: 4 ['crane', 'noun', 'A tall, long-legged, long-necked bird, typically with white or gray plumage and often with tail feathers that are expanded into a fan.', 'The white crane gracefully initiated its flight towards the horizon.', 'crane', 'noun', 'A large, tall machine used for moving heavy objects by suspending them from a beam.', 'The construction site was bustling with workers and the steady movement of the crane.', 'crane', 'verb', \"To stretch out one's body or neck in order to see something.\", 'The audience had to crane their necks to get a better view of the stage.', 'crane', 'verb', 'To lift and move (heavy objects) with a crane.', 'The workers had to crane the large steel beams onto the construction site.']\n",
      "GEN: 1th word: crane\n",
      "GEN: 2th word: crane\n",
      "GEN: 3th word: crane\n",
      "GEN: 4th word: crane\n",
      "1 computer\n",
      "GEN: meanings, size: 2 ['computer', 'noun', 'A machine for performing calculations automatically.', 'She used her computer to calculate the equations.', 'An expert at calculation (or at operating calculating machines).', 'The mathematician was such a human computer, able to solve complex equations swiftly.', 'A device that accepts information (in the form of digitalized data) and manipulates it for some result based on a program or sequence of instructions on how the data is to be processed.', 'My computer crashed and I lost all my documents.']\n",
      "GEN: 1th word: computer\n",
      "GEN: 2th word: An expert at calculation (or at operating calculating machines).\n",
      "2 university\n",
      "GEN: meanings, size: 2 ['University', 'Noun', 'An institution of higher education (typically providing a level of education more than a high school) where you can study for an undergraduate or postgraduate degree.', 'She is planning to enroll in the University of Oxford next year.', 'The body of faculty and students at a university.', 'The entire university was excited for the annual sports event.', 'A large and diverse place of learning, creating, and exploring knowledge.', 'The university opened a whole new world of knowledge to him.']\n",
      "GEN: 1th word: University\n",
      "GEN: 2th word: The body of faculty and students at a university.\n",
      "3 tongue\n",
      "GEN: meanings, size: 5 ['Tongue', 'Noun', 'The fleshy muscular organ in the mouth of a mammal, used for tasting, licking, swallowing, and (in humans) articulating speech.', 'She bit her tongue and tasted blood.', 'Tongue', 'Noun', 'A particular way or manner of speaking.', 'He had a strong dialect, his mother tongue was not English.', 'Tongue', 'Noun', 'A strip of leather or other material under the lacing or fastening of a shoe.', 'I had trouble tying my shoe due to the oversized tongue.', 'Tongue', 'Verb', 'Articulate (notes on a wind instrument) by interrupting the air flow with the tongue.', 'The oboist had a unique style, and would tongue the notes with precision.', 'Tongue', 'Verb', 'Lick or caress with the tongue.', 'The cat tongue its fur to clean itself.']\n",
      "GEN: 1th word: Tongue\n",
      "GEN: 2th word: Tongue\n",
      "GEN: 3th word: Tongue\n",
      "GEN: 4th word: Tongue\n",
      "GEN: 5th word: Tongue\n",
      "4 carrot\n",
      "5 jumping\n",
      "GEN: meanings, size: 4 ['jumping', 'adjective', 'Briskly active or alert.', '\"The jumping child was full of energy.\"', 'jumping', 'verb', 'Propelling oneself rapidly upward, downward, or in any horizontal direction such that momentum and muscular effort are used to project oneself into the air.', '\"The kangaroo is jumping over the fence.\"', 'jumping', 'noun', 'The act of participating in an athletic competition featuring this mode of locomotion.', '\"He is known for his high jumping skills.\"', 'jumping', 'adjective', 'Unexpectedly changing from one state or activity to another.', '\"He made a jumping transition from one topic to another.\"']\n",
      "GEN: 1th word: jumping\n",
      "GEN: 2th word: jumping\n",
      "GEN: 3th word: jumping\n",
      "GEN: 4th word: jumping\n",
      "6 bark\n",
      "GEN: meanings, size: 4 ['Bark', 'Noun', 'The tough protective outer sheath of the trunk, branches, and twigs of a tree or woody shrub.', 'The tree had a rough bark.', 'Bark', 'Noun', 'The sound a dog makes.', \"The dog's bark is louder than its bite.\", 'Bark', 'Verb', '(of a dog or other animal) to make a characteristic short loud cry.', \"The neighbor's dog barked all night.\", 'Bark', 'Verb', 'To say something in a loud, harsh voice.', 'The sergeant barked an order at the soldiers.']\n",
      "GEN: 1th word: Bark\n",
      "GEN: 2th word: Bark\n",
      "GEN: 3th word: Bark\n",
      "GEN: 4th word: Bark\n",
      "7 headphones\n",
      "GEN: meanings, size: 1 ['Headphones', 'Nouns', \"a pair of small loudspeaker drivers worn on or around the head over a user's ears.\", 'He put on his headphones and lost himself in the music.']\n",
      "GEN: 1th word: Headphones\n",
      "8 frog\n",
      "GEN: meanings, size: 6 ['Frog', 'Noun', 'Any of various tailless stout-bodied amphibians with long hind limbs for leaping; semiaquatic and terrestrial species.', 'The pond is teeming with frogs.', 'Frog', 'Noun', 'A person of French descent.', 'He had a friendly chat with a Frog while traveling in France.', 'Frog', 'Noun', 'A strip of leather or rubber in the heel of a shoe to make it non slip.', 'Make sure your horseshoes have a frog to prevent the horse from slipping.', 'Frog', 'Noun', 'The elastic horny pad in the middle of the sole of the foot of a horse or similar animal.', \"The vet examined the frog of the horse's hoof.\", 'Frog', 'Noun', 'An ornamental looped braid or cord with a button or knot that fastens the front of a garment.', 'She adorned her dress with an intricate frog closure.', 'Frog', 'Noun', 'A machine component which redirects a motion.', 'The engineer adjusted the frog of the rail switch to redirect the train.']\n",
      "GEN: 1th word: Frog\n",
      "GEN: 2th word: Frog\n",
      "GEN: 3th word: Frog\n",
      "GEN: 4th word: Frog\n",
      "GEN: 5th word: Frog\n",
      "GEN: 6th word: Frog\n",
      "9 flowers\n",
      "GEN: meanings, size: 4 ['Flowers', 'Nouns', 'The seed-bearing part of a plant, consisting of reproductive organs (stamens and carpels) that are typically surrounded by a brightly colored corolla (petals) and a green calyx (sepals).', 'He brought her a bouquet of beautiful flowers for their anniversary.', 'Flowers', 'Nouns', 'The finest individuals out of a number of people or things.', 'She was considered one of the flowers of her generation because of her intelligence and beauty.', 'Flowers', 'Verbs', 'To develop or come to a promising stage.', 'Their friendship flowered into a lifelong bond.', 'Flowers', 'Verbs', '(of a plant) Produce flowers; bloom.', 'The plants will flower in the spring.']\n",
      "GEN: 1th word: Flowers\n",
      "GEN: 2th word: Flowers\n",
      "GEN: 3th word: Flowers\n",
      "GEN: 4th word: Flowers\n"
     ]
    }
   ],
   "source": [
    "is_test = True\n",
    "cur_time = datetime.datetime.now()\n",
    "file_name = f'{cur_time}_random_words.txt'\n",
    "if not is_test:\n",
    "    rand_lines_nr = 5\n",
    "    with open('words.txt') as fp:\n",
    "    # with open('lemmad.txt') as fp:\n",
    "        rand_lines = random.sample(list(fp), rand_lines_nr)\n",
    "    with open(file_name, 'w') as fp:\n",
    "        fp.writelines(rand_lines)\n",
    "    r = main(input_file=file_name, cur_time=cur_time, is_est=False)\n",
    "else:\n",
    "    r = main(input_file='test.txt', cur_time=cur_time, is_est=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46566\n"
     ]
    }
   ],
   "source": [
    "print(total_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(input_file):\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        # Get generated and actual synsets and compare relations by calculating the overlap\n",
    "        for synset in root.findall('synset'):\n",
    "            generated = synset.find('generated')\n",
    "            actual = synset.find('actual')\n",
    "            print(f\"Word: {synset.get('word')}, Type: {synset.get('type')}\")\n",
    "            print(\n",
    "                f\"Generated: {generated.find('meaning').text}, Actual: {actual.find('meaning').text}\")\n",
    "            for relation in relations:\n",
    "                gen_rel = generated.find(f\"{relation}s\")\n",
    "                act_rel = actual.find(f\"{relation}s\")\n",
    "                if gen_rel is None and act_rel is None:\n",
    "                    continue\n",
    "                if gen_rel is None or act_rel is None:\n",
    "                    print(\n",
    "                        f\"Relation: {relation}, Gen: {gen_rel}, Act: {act_rel}\")\n",
    "                    continue\n",
    "                gen_rel = set(gen_rel.text.split())\n",
    "                act_rel = set(act_rel.text.split())\n",
    "                print(f\"Relation: {relation}, Gen: {gen_rel}, Act: {act_rel}\")\n",
    "                print(\n",
    "                    f\"Overlap: {len(gen_rel.intersection(act_rel))}, Gen: {len(gen_rel)}, Act: {len(act_rel)}\")\n",
    "\n",
    "\n",
    "def count_total_stats(input_file):\n",
    "    with open(input_file, 'r') as fp:\n",
    "        xml_file = ET.parse(fp)\n",
    "        root = xml_file.getroot()\n",
    "        total_gen = 0\n",
    "        total_actual = 0\n",
    "        total_overlapping = 0\n",
    "        total_over_generated = 0\n",
    "        total_under_generated = 0\n",
    "        for synset in root.findall('synset'):\n",
    "            stats = synset.find('stats')\n",
    "            total_stats = stats.find('total')\n",
    "            total_gen += int(total_stats.find('generated_size').text)\n",
    "            total_actual += int(total_stats.find('actual_size').text)\n",
    "            total_overlapping += int(total_stats.find('overlapping').text)\n",
    "            total_over_generated += int(total_stats.find('over_generated').text)\n",
    "            total_under_generated += int(\n",
    "                total_stats.find('under_generated').text)\n",
    "        print(f\"Total: Gen: {total_gen}, Act: {total_actual}, Overlap: {total_overlapping}, Over Gen: {total_over_generated}, Under Gen: {total_under_generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: Gen: 346, Act: 27, Overlap: 9, Over Gen: 337, Under Gen: 18\n"
     ]
    }
   ],
   "source": [
    "count_total_stats(f'{cur_time}_output.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
